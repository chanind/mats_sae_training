{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#saelens","title":"SAELens","text":"<p>The SAELens training codebase exists to help researchers:</p> <ul> <li>Train sparse autoencoders.</li> <li>Analyse sparse autoencoders and neural network internals.</li> <li>Generate insights which make it easier to create safe and aligned AI systems.</li> </ul> <p>Please note these docs are in beta. We intend to make them cleaner and more comprehensive over time.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install sae-lens\n</code></pre>"},{"location":"#loading-sparse-autoencoders-from-huggingface","title":"Loading Sparse Autoencoders from Huggingface","text":"<p>Previously trained sparse autoencoders can be loaded from huggingface with close to single line of code. For more details and performance metrics for these sparse autoencoder, read my blog post. </p> <pre><code>import torch \nfrom sae_lens import LMSparseAutoencoderSessionloader\nfrom huggingface_hub import hf_hub_download\n\nlayer = 8 # pick a layer you want.\nREPO_ID = \"jbloom/GPT2-Small-SAEs\"\nFILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{layer}.hook_resid_pre_24576.pt\"\npath = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\nmodel, sparse_autoencoder, activation_store = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n    path = path\n)\nsparse_autoencoder.eval()\n</code></pre> <p>You can also load the feature sparsity from huggingface. </p> <pre><code>FILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{layer}.hook_resid_pre_24576_log_feature_sparsity.pt\"\npath = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\nlog_feature_sparsity = torch.load(path, map_location=sparse_autoencoder.cfg.device)\n</code></pre>"},{"location":"#background","title":"Background","text":"<p>We highly recommend this tutorial.</p>"},{"location":"#code-overview","title":"Code Overview","text":"<p>The codebase contains 2 folders worth caring about:</p> <ul> <li>training: The main body of the code is here. Everything required for training SAEs. </li> <li>analysis: This code is mainly house the feature visualizer code we use to generate dashboards. It was written by Callum McDougal but I've ported it here with permission and edited it to work with a few different activation types. </li> </ul> <p>Some other folders:</p> <ul> <li>tutorials: These aren't well maintained but I'll aim to clean them up soon. </li> <li>tests: When first developing the codebase, I was writing more tests. I have no idea whether they are currently working!</li> </ul>"},{"location":"#loading-a-pretrained-language-model","title":"Loading a Pretrained Language Model","text":"<p>Once your SAE is trained, the final SAE weights will be saved to wandb and are loadable via the session loader. The session loader will return: - The model your SAE was trained on (presumably you're interested in studying this. It's always a HookedTransformer) - Your SAE. - An activations loader: from which you can get randomly sampled activations or batches of tokens from the dataset you used to train the SAE. (more on this in the tutorial)</p> <pre><code>from sae_lens import LMSparseAutoencoderSessionloader\n\npath =\"path/to/sparse_autoencoder.pt\"\nmodel, sparse_autoencoder, activations_loader = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n    path\n)\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"<p>I wrote a tutorial to show users how to do some basic exploration of their SAE:</p> <ul> <li><code>evaluating_your_sae.ipynb</code>: A quick/dirty notebook showing how to check L0 and Prediction loss with your SAE, as well as showing how to generate interactive dashboards using Callum's reporduction of Anthropics interface.</li> <li><code>logits_lens_with_features.ipynb</code>: A notebook showing how to reproduce the analysis from this LessWrong post.</li> </ul>"},{"location":"#example-dashboard","title":"Example Dashboard","text":"<p>WandB Dashboards provide lots of useful insights while training SAE's. Here's a screenshot from one training run. </p> <p></p>"},{"location":"#citations-and-references","title":"Citations and References:","text":"<p>Research: - Towards Monosemanticy - Sparse Autoencoders Find Highly Interpretable Features in Language Model</p> <p>Reference Implementations: - Neel Nanda - AI-Safety-Foundation. - Arthur Conmy. - Callum McDougall</p>"},{"location":"feature_dashboards/","title":"Feature dashboards","text":""},{"location":"feature_dashboards/#example-output","title":"Example Output","text":"<p>Here's one feature we found in the residual stream of Layer 10 of GPT-2 Small:</p> <p>. Open <code>gpt2_resid_pre10_predict_pronoun_feature.html</code> in your browser to interact with the dashboard (WIP).</p> <p>Note, probably this feature could split into more mono-semantic features in a larger SAE that had been trained for longer. (this was was only about 49152 features trained on 10M tokens from OpenWebText).</p>"},{"location":"installation/","title":"Installation","text":"<p>This package is available on PyPI. You can install it via pip:</p> <pre><code>pip install sae-lens\n</code></pre>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#motivation","title":"Motivation","text":"<ul> <li>Accelerate SAE Research: Support fast experimentation to understand SAEs and improve SAE training so we can train SAEs on larger and more diverse models.</li> <li>Make Research like Play: Support research into language model internals via SAEs. Good tooling can make research tremendously exciting and enjoyable. Balancing modifiability and reliability with ease of understanding / access is the name of the game here.</li> <li>Build an awesome community: Mechanistic Interpretability already has an awesome community but as that community grows, it makes sense that there will be niches. I'd love to build a great community around Sparse Autoencoders.</li> </ul>"},{"location":"roadmap/#goals","title":"Goals","text":""},{"location":"roadmap/#sae-training","title":"SAE Training","text":"<p>SAE Training features will fit into a number of categories including:</p> <ul> <li>Making it easy to train SAEs: Training SAEs is hard for a number of reasons and so making it easy for people to train SAEs with relatively little expertise seems like the main way this codebase will create value. </li> <li>Training SAEs on more models: Supporting training of SAEs on more models, architectures, different activations within those models.</li> <li>Being better at training SAEs: Enabling methodological changes which may improve SAE performance as measured by reconstruction loss, Cross Entropy Loss when using reconstructed activation, L1 loss, L0 and interpretability of features as well as improving speed of training or reducing the compute resources required to train SAEs. </li> <li>Being better at measuring SAE Performance: How do we know when SAEs are doing what we want them to? Improving training metrics should allow better decisions about which methods to use and which hyperparameters choices we make.</li> <li>Training SAE variants: People are already training \u201cTranscoders\u201d which map from one activation to another (such as before / after an MLP layer). These can be easily supported with a few changes. Other variants will come in time and </li> </ul>"},{"location":"roadmap/#analysis-with-saes","title":"Analysis with SAEs","text":"<p>Using SAEs to understand neural network internals is an exciting, but complicated task.</p> <ul> <li>Feature-wise Interpretability: This looks something like \"for each feature, have as much knowledge about it as possible\". Part of this will feature dashboard improvements, or supporting better integrations with Neuronpedia.</li> <li>Mechanistic Interpretability: This comprises the more traditional kinds of Mechanistic Interpretability which TransformerLens supports and should be supported by this codebase. Making it easy to patch, ablate or otherwise intervene on features so as to find circuits will likely speed up lots of researchers.</li> </ul>"},{"location":"roadmap/#other-stuff","title":"Other Stuff","text":"<p>I think there are lots of other types of analysis that could be done in the future with SAE features. I've already explored many different types of statistical tests which can reveal interesting properties of features. There are also things like saliency mapping and attribution techniques which it would be nice to support.</p> <ul> <li>Accessibility and Code Quality: The codebase won\u2019t be used if it doesn\u2019t work and it also won\u2019t get used if it\u2019s too hard to understand, modify or read.  Making the code accessible: This involves tasks like turning the code base into a python package.</li> <li>Knowing how the code is supposed to work: Is the code well-documented? This will require docstrings, tutorials and links to related work and publications. Getting aligned on what the code does is critical to sharing a resource like this. </li> <li>Knowing the code works as intended: All code should be tested. Unit tests and acceptance tests are both important.</li> <li>Knowing the code is actually performant: This will ensure code works as intended. However deep learning introduces lots of complexity which makes actually running benchmarks essential to having confidence in the code. </li> </ul>"},{"location":"training_saes/","title":"Training Sparse Autoencoders","text":"<p>Sparse Autoencoders can be intimidating at first but it's fairly simple to train one once you know what each part of the config does. I've created a config class which you instantiate and pass to the runner which will complete your training run and log it's progress to wandb. </p> <p>Let's go through the major components of the config:</p> <ul> <li>Data: SAE's autoencode model activations. We need to specify the model, the part of the models activations we want to autoencode and the dataset the model is operating on when generating those activations. We now automatically detect if that dataset is tokenized and most huggingface datasets should be fine. One slightly annoying detail is that you need to know the dimensionality of those activations when contructing your SAE but you can get that in the transformerlens docs. Any language model in the table from those docs should work. </li> <li>SAE Parameters: Your expansion factor will determine the size of your SAE and the decoder bias initialization method should always be geometric_median or mean. Mean is faster but theoretically sub-optimal. I use another package to get the geometric median and it can be quite slow. </li> <li>Training Parameters: These are most critical. The right L1 coefficient (coefficient in the activation sparsity inducing term in the loss) changes with your learning rate but a good bet would be to use LR 4e-4 and L1 8e-5 for GPT2 small. These will vary for other models and playing around with them / short runs can be helpful. Training batch size of 4096 is standard and I'm not really sure whether there's benefit to playing with it. In theory a larger context size (one accurate to whatever the model was trained with) seems good but it's computationally cheaper to use 128. Learning rate warm up is important to avoid dead neurons. </li> <li>Activation Store Parameters: The activation store shuffles activations from forward passes over samples from your data. The larger it is, the better shuffling you'll get. In theory more shuffling is good. The total training tokens is a very important parameter. The more the better, but you'll often see good results having trained on a few hundred million tokens. Store batch batch size is a function of your gpu and how many forward passes of your model you want to do simultaneously when collecting activations.</li> <li>Dead Neurons / Sparsity Metrics: The config around resampling was more important when we were using resampling to avoid dead neurons (see Anthropic's post on this), but using ghost gradients, the resampling protcol is much simpler. I'd always set ghost grad to True and feature sampling method to None. The feature sampling window effects the dashboard statistics tracking feature occurence and the dead feature window tracks how many forward passes a neuron must not activate before we apply ghost grads to it. </li> <li>WANDB: Fairly straightfoward. Don't set log frequency too high or your dashboard will be slow!</li> <li>Device: I can run this code on my macbook with \"mps\" but mostly do runs with cuda. </li> <li>Dtype: Float16 maybe could work but I had some funky results and have left it at float32 for the time being. </li> <li>Checkpoints: I'd collected checkpoints on runs you care about but turn them off when tuning since it can be slow. </li> </ul> <pre><code>import torch\nimport os \nimport sys \n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"WANDB__SERVICE_WAIT\"] = \"300\"\n\nfrom sae_lens import LanguageModelSAERunnerConfig, language_model_sae_runner\n\ncfg = LanguageModelSAERunnerConfig(\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name = \"gpt2-small\",\n    hook_point = \"blocks.2.hook_resid_pre\",\n    hook_point_layer = 2,\n    d_in = 768,\n    dataset_path = \"Skylion007/openwebtext\",\n    is_dataset_tokenized=False,\n\n    # SAE Parameters\n    expansion_factor = 64,\n    b_dec_init_method = \"geometric_median\",\n\n    # Training Parameters\n    lr = 0.0004,\n    l1_coefficient = 0.00008,\n    lr_scheduler_name=\"constantwithwarmup\",\n    train_batch_size = 4096,\n    context_size = 128,\n    lr_warm_up_steps=5000,\n\n    # Activation Store Parameters\n    n_batches_in_buffer = 128,\n    training_tokens = 1_000_000 * 300,\n    store_batch_size = 32,\n\n    # Dead Neurons and Sparsity\n    use_ghost_grads=True,\n    feature_sampling_window = 1000,\n    dead_feature_window=5000,\n    dead_feature_threshold = 1e-6,\n\n    # WANDB\n    log_to_wandb = True,\n    wandb_project= \"gpt2\",\n    wandb_entity = None,\n    wandb_log_frequency=100,\n\n    # Misc\n    device = \"cuda\",\n    seed = 42,\n    n_checkpoints = 10,\n    checkpoint_path = \"checkpoints\",\n    dtype = torch.float32,\n    )\n\nsparse_autoencoder = language_model_sae_runner(cfg)\n</code></pre>"},{"location":"about/citation/","title":"Citation","text":"<pre><code>@misc{bloom2024saetrainingcodebase,\n   title = {SAELens Training\n   author = {Joseph Bloom},\n   year = {2024},\n   howpublished = {\\url{}},\n}}\n</code></pre>"},{"location":"about/contributing/","title":"Contributing","text":"<p>Contributions are welcome! To get setup for development, follow the instructions below.</p>"},{"location":"about/contributing/#setup","title":"Setup","text":"<p>Make sure you have poetry installed, clone the repository, and install dependencies with:</p> <pre><code>poetry install\n</code></pre>"},{"location":"about/contributing/#testing-linting-and-formatting","title":"Testing, Linting, and Formatting","text":"<p>This project uses pytest for testing, flake8 for linting, pyright for type-checking, and black and isort for formatting.</p> <p>If you add new code, it would be greatly appreciated if you could add tests in the <code>tests/unit</code> directory. You can run the tests with:</p> <pre><code>make unit-test\n</code></pre> <p>Before commiting, make sure you format the code with:</p> <pre><code>make format\n</code></pre> <p>Finally, run all CI checks locally with:</p> <pre><code>make check-ci\n</code></pre> <p>If these pass, you're good to go! Open a pull request with your changes.</p>"},{"location":"about/contributing/#documentation","title":"Documentation","text":"<p>This project uses mkdocs for documentation. You can see the docs locally with:</p> <p><pre><code>make docs-serve\n</code></pre> If you make changes to code which requires updating documentation, it would be greatly appreciated if you could update the docs as well.</p>"},{"location":"reference/language_models/","title":"Language Models","text":"<p>Most of this is just copied over from Arthur's code and slightly simplified: https://github.com/ArthurConmy/sae/blob/main/sae/model.py</p>"},{"location":"reference/language_models/#sae_lens.training.lm_runner.language_model_sae_runner","title":"<code>language_model_sae_runner(cfg)</code>","text":"Source code in <code>sae_lens/training/lm_runner.py</code> <pre><code>def language_model_sae_runner(cfg: LanguageModelSAERunnerConfig):\n    \"\"\" \"\"\"\n\n    if cfg.from_pretrained_path is not None:\n        (\n            model,\n            sparse_autoencoder,\n            activations_loader,\n        ) = LMSparseAutoencoderSessionloader.load_pretrained_sae(\n            cfg.from_pretrained_path\n        )\n        cfg = sparse_autoencoder.cfg\n    else:\n        loader = LMSparseAutoencoderSessionloader(cfg)\n        model, sparse_autoencoder, activations_loader = (\n            loader.load_sae_training_group_session()\n        )\n\n    if cfg.log_to_wandb:\n        wandb.init(project=cfg.wandb_project, config=cast(Any, cfg), name=cfg.run_name)\n\n    # train SAE\n    sparse_autoencoder = train_sae_on_language_model(\n        model,\n        sparse_autoencoder,\n        activations_loader,\n        n_checkpoints=cfg.n_checkpoints,\n        batch_size=cfg.train_batch_size,\n        feature_sampling_window=cfg.feature_sampling_window,\n        dead_feature_threshold=cfg.dead_feature_threshold,\n        use_wandb=cfg.log_to_wandb,\n        wandb_log_frequency=cfg.wandb_log_frequency,\n    )\n\n    if cfg.log_to_wandb:\n        wandb.finish()\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.train_sae_on_language_model.SAETrainContext","title":"<code>SAETrainContext</code>  <code>dataclass</code>","text":"<p>Context to track during training for a single SAE</p> Source code in <code>sae_lens/training/train_sae_on_language_model.py</code> <pre><code>@dataclass\nclass SAETrainContext:\n    \"\"\"\n    Context to track during training for a single SAE\n    \"\"\"\n\n    act_freq_scores: torch.Tensor\n    n_forward_passes_since_fired: torch.Tensor\n    n_frac_active_tokens: int\n    optimizer: Optimizer\n    scheduler: LRScheduler\n    finetuning: bool = False\n\n    @property\n    def feature_sparsity(self) -&gt; torch.Tensor:\n        return self.act_freq_scores / self.n_frac_active_tokens\n\n    @property\n    def log_feature_sparsity(self) -&gt; torch.Tensor:\n        return _log_feature_sparsity(self.feature_sparsity)\n\n    def begin_finetuning(self, sae: SparseAutoencoder):\n\n        # finetuning method should be set in the config\n        # if not, then we don't finetune\n        if not isinstance(sae.cfg.finetuning_method, str):\n            return\n\n        for name, param in sae.named_parameters():\n            if name in FINETUNING_PARAMETERS[sae.cfg.finetuning_method]:\n                param.requires_grad = True\n            else:\n                param.requires_grad = False\n\n        self.finetuning = True\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.train_sae_on_language_model.train_sae_on_language_model","title":"<code>train_sae_on_language_model(model, sae_group, activation_store, batch_size=1024, n_checkpoints=0, feature_sampling_window=1000, dead_feature_threshold=1e-08, use_wandb=False, wandb_log_frequency=50)</code>","text":"<p>@deprecated Use <code>train_sae_group_on_language_model</code> instead. This method is kept for backward compatibility.</p> Source code in <code>sae_lens/training/train_sae_on_language_model.py</code> <pre><code>def train_sae_on_language_model(\n    model: HookedRootModule,\n    sae_group: SparseAutoencoderDictionary,\n    activation_store: ActivationsStore,\n    batch_size: int = 1024,\n    n_checkpoints: int = 0,\n    feature_sampling_window: int = 1000,  # how many training steps between resampling the features / considiring neurons dead\n    dead_feature_threshold: float = 1e-8,  # how infrequently a feature has to be active to be considered dead\n    use_wandb: bool = False,\n    wandb_log_frequency: int = 50,\n) -&gt; SparseAutoencoderDictionary:\n    \"\"\"\n    @deprecated Use `train_sae_group_on_language_model` instead. This method is kept for backward compatibility.\n    \"\"\"\n    return train_sae_group_on_language_model(\n        model,\n        sae_group,\n        activation_store,\n        batch_size,\n        n_checkpoints,\n        feature_sampling_window,\n        use_wandb,\n        wandb_log_frequency,\n    ).sae_group\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder","title":"<code>SparseAutoencoder</code>","text":"<p>             Bases: <code>HookedRootModule</code></p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>class SparseAutoencoder(HookedRootModule):\n    \"\"\" \"\"\"\n\n    l1_coefficient: float\n    lp_norm: float\n    d_sae: int\n    use_ghost_grads: bool\n    normalize_sae_decoder: bool\n    hook_point_layer: int\n    dtype: torch.dtype\n    device: str | torch.device\n    noise_scale: float\n    activation_fn: Callable[[torch.Tensor], torch.Tensor]\n\n    def __init__(\n        self,\n        cfg: LanguageModelSAERunnerConfig,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.d_in = cfg.d_in\n        if not isinstance(self.d_in, int):\n            raise ValueError(\n                f\"d_in must be an int but was {self.d_in=}; {type(self.d_in)=}\"\n            )\n        assert cfg.d_sae is not None  # keep pyright happy\n        # lists are valid only for SAEGroup cfg, not SAE cfg vals\n        assert not isinstance(cfg.l1_coefficient, list)\n        assert not isinstance(cfg.lp_norm, list)\n        assert not isinstance(cfg.lr, list)\n        assert not isinstance(cfg.lr_scheduler_name, list)\n        assert not isinstance(cfg.lr_warm_up_steps, list)\n        assert not isinstance(cfg.use_ghost_grads, list)\n        assert not isinstance(cfg.hook_point_layer, list)\n        assert (\n            \"{layer}\" not in cfg.hook_point\n        ), \"{layer} must be replaced with the actual layer number in SAE cfg\"\n\n        self.d_sae = cfg.d_sae\n        self.l1_coefficient = cfg.l1_coefficient\n        self.lp_norm = cfg.lp_norm\n        self.dtype = cfg.dtype\n        self.device = cfg.device\n        self.use_ghost_grads = cfg.use_ghost_grads\n        self.normalize_sae_decoder = cfg.normalize_sae_decoder\n        self.hook_point_layer = cfg.hook_point_layer\n        self.noise_scale = cfg.noise_scale\n        self.activation_fn = get_activation_fn(cfg.activation_fn)\n\n        # NOTE: if using resampling neurons method, you must ensure that we initialise the weights in the order W_enc, b_enc, W_dec, b_dec\n        self.W_enc = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(self.d_in, self.d_sae, dtype=self.dtype, device=self.device)\n            )\n        )\n        self.b_enc = nn.Parameter(\n            torch.zeros(self.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n        self.W_dec = nn.Parameter(\n            torch.nn.init.kaiming_uniform_(\n                torch.empty(self.d_sae, self.d_in, dtype=self.dtype, device=self.device)\n            )\n        )\n\n        if self.cfg.decoder_orthogonal_init:\n            self.W_dec.data = nn.init.orthogonal_(self.W_dec.data.T).T\n\n        if self.normalize_sae_decoder:\n            with torch.no_grad():\n                # Anthropic normalize this to have unit columns\n                self.set_decoder_norm_to_unit_norm()\n\n        self.b_dec = nn.Parameter(\n            torch.zeros(self.d_in, dtype=self.dtype, device=self.device)\n        )\n\n        # scaling factor for fine-tuning (not to be used in initial training)\n        self.scaling_factor = nn.Parameter(\n            torch.ones(self.d_sae, dtype=self.dtype, device=self.device)\n        )\n\n        self.hook_sae_in = HookPoint()\n        self.hook_hidden_pre = HookPoint()\n        self.hook_hidden_post = HookPoint()\n        self.hook_sae_out = HookPoint()\n\n        self.setup()  # Required for `HookedRootModule`s\n\n    def forward(self, x: torch.Tensor, dead_neuron_mask: torch.Tensor | None = None):\n        # move x to correct dtype\n        x = x.to(self.dtype)\n        sae_in = self.hook_sae_in(\n            x - (self.b_dec * self.cfg.apply_b_dec_to_input)\n        )  # Remove decoder bias as per Anthropic\n\n        hidden_pre = self.hook_hidden_pre(\n            einops.einsum(\n                sae_in,\n                self.W_enc,\n                \"... d_in, d_in d_sae -&gt; ... d_sae\",\n            )\n            + self.b_enc\n        )\n        noisy_hidden_pre = hidden_pre\n        if self.noise_scale &gt; 0:\n            noise = torch.randn_like(hidden_pre) * self.noise_scale\n            noisy_hidden_pre = hidden_pre + noise\n        feature_acts = self.hook_hidden_post(self.activation_fn(noisy_hidden_pre))\n\n        sae_out = self.hook_sae_out(\n            einops.einsum(\n                feature_acts\n                * self.scaling_factor,  # need to make sure this handled when loading old models.\n                self.W_dec,\n                \"... d_sae, d_sae d_in -&gt; ... d_in\",\n            )\n            + self.b_dec\n        )\n\n        # add config for whether l2 is normalized:\n        per_item_mse_loss = _per_item_mse_loss_with_target_norm(\n            sae_out, x, self.cfg.mse_loss_normalization\n        )\n        ghost_grad_loss = torch.tensor(0.0, dtype=self.dtype, device=self.device)\n        # gate on config and training so evals is not slowed down.\n        if (\n            self.use_ghost_grads\n            and self.training\n            and dead_neuron_mask is not None\n            and dead_neuron_mask.sum() &gt; 0\n        ):\n            ghost_grad_loss = self.calculate_ghost_grad_loss(\n                x=x,\n                sae_out=sae_out,\n                per_item_mse_loss=per_item_mse_loss,\n                hidden_pre=hidden_pre,\n                dead_neuron_mask=dead_neuron_mask,\n            )\n\n        mse_loss = per_item_mse_loss.mean()\n        sparsity = feature_acts.norm(p=self.lp_norm, dim=1).mean(dim=(0,))\n        l1_loss = self.l1_coefficient * sparsity\n        loss = mse_loss + l1_loss + ghost_grad_loss\n\n        return ForwardOutput(\n            sae_out=sae_out,\n            feature_acts=feature_acts,\n            loss=loss,\n            mse_loss=mse_loss,\n            l1_loss=l1_loss,\n            ghost_grad_loss=ghost_grad_loss,\n        )\n\n    @torch.no_grad()\n    def initialize_b_dec_with_precalculated(self, origin: torch.Tensor):\n        out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n        self.b_dec.data = out\n\n    @torch.no_grad()\n    def initialize_b_dec_with_mean(self, all_activations: torch.Tensor):\n        previous_b_dec = self.b_dec.clone().cpu()\n        out = all_activations.mean(dim=0)\n\n        previous_distances = torch.norm(all_activations - previous_b_dec, dim=-1)\n        distances = torch.norm(all_activations - out, dim=-1)\n\n        print(\"Reinitializing b_dec with mean of activations\")\n        print(\n            f\"Previous distances: {previous_distances.median(0).values.mean().item()}\"\n        )\n        print(f\"New distances: {distances.median(0).values.mean().item()}\")\n\n        self.b_dec.data = out.to(self.dtype).to(self.device)\n\n    @torch.no_grad()\n    def set_decoder_norm_to_unit_norm(self):\n        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)\n\n    @torch.no_grad()\n    def remove_gradient_parallel_to_decoder_directions(self):\n        \"\"\"\n        Update grads so that they remove the parallel component\n            (d_sae, d_in) shape\n        \"\"\"\n        assert self.W_dec.grad is not None  # keep pyright happy\n\n        parallel_component = einops.einsum(\n            self.W_dec.grad,\n            self.W_dec.data,\n            \"d_sae d_in, d_sae d_in -&gt; d_sae\",\n        )\n        self.W_dec.grad -= einops.einsum(\n            parallel_component,\n            self.W_dec.data,\n            \"d_sae, d_sae d_in -&gt; d_sae d_in\",\n        )\n\n    def save_model_legacy(self, path: str):\n        \"\"\"\n        Basic save function for the model. Saves the model's state_dict and the config used to train it.\n        \"\"\"\n\n        # check if path exists\n        folder = os.path.dirname(path)\n        os.makedirs(folder, exist_ok=True)\n\n        state_dict = {\"cfg\": self.cfg, \"state_dict\": self.state_dict()}\n\n        if path.endswith(\".pt\"):\n            torch.save(state_dict, path)\n        else:\n            raise ValueError(\n                f\"Unexpected file extension: {path}, supported extensions are .pt and .pkl.gz\"\n            )\n\n        print(f\"Saved model to {path}\")\n\n    def save_model(self, path: str, sparsity: Optional[torch.Tensor] = None):\n\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n        # generate the weights\n        save_file(self.state_dict(), f\"{path}/sae_weights.safetensors\")\n\n        # save the config\n        config = {\n            **self.cfg.__dict__,\n            # some args may not be serializable by default\n            \"dtype\": str(self.cfg.dtype),\n            \"device\": str(self.cfg.device),\n        }\n\n        with open(f\"{path}/cfg.json\", \"w\") as f:\n            json.dump(config, f)\n\n        if sparsity is not None:\n            sparsity_in_dict = {\"sparsity\": sparsity}\n            save_file(sparsity_in_dict, f\"{path}/sparsity.safetensors\")  # type: ignore\n\n    @classmethod\n    def load_from_pretrained_legacy(cls, path: str):\n        \"\"\"\n        Load function for the model. Loads the model's state_dict and the config used to train it.\n        This method can be called directly on the class, without needing an instance.\n        \"\"\"\n\n        # Ensure the file exists\n        if not os.path.isfile(path):\n            raise FileNotFoundError(f\"No file found at specified path: {path}\")\n\n        # Load the state dictionary\n        if path.endswith(\".pt\"):\n            try:\n                if torch.backends.mps.is_available():\n                    state_dict = torch.load(\n                        path,\n                        map_location=\"mps\",\n                        pickle_module=BackwardsCompatiblePickleClass,\n                    )\n                    state_dict[\"cfg\"].device = \"mps\"\n                else:\n                    state_dict = torch.load(\n                        path, pickle_module=BackwardsCompatiblePickleClass\n                    )\n            except Exception as e:\n                raise IOError(f\"Error loading the state dictionary from .pt file: {e}\")\n        elif path.endswith(\".pkl.gz\"):\n            try:\n                with gzip.open(path, \"rb\") as f:\n                    state_dict = pickle.load(f)\n            except Exception as e:\n                raise IOError(\n                    f\"Error loading the state dictionary from .pkl.gz file: {e}\"\n                )\n        elif path.endswith(\".pkl\"):\n            try:\n                with open(path, \"rb\") as f:\n                    state_dict = pickle.load(f)\n            except Exception as e:\n                raise IOError(f\"Error loading the state dictionary from .pkl file: {e}\")\n        else:\n            raise ValueError(\n                f\"Unexpected file extension: {path}, supported extensions are .pt, .pkl, and .pkl.gz\"\n            )\n\n        # Ensure the loaded state contains both 'cfg' and 'state_dict'\n        if \"cfg\" not in state_dict or \"state_dict\" not in state_dict:\n            raise ValueError(\n                \"The loaded state dictionary must contain 'cfg' and 'state_dict' keys\"\n            )\n\n        # Create an instance of the class using the loaded configuration\n        instance = cls(cfg=state_dict[\"cfg\"])\n        if \"scaling_factor\" not in state_dict[\"state_dict\"]:\n            assert isinstance(instance.cfg.d_sae, int)\n            state_dict[\"state_dict\"][\"scaling_factor\"] = torch.ones(\n                instance.cfg.d_sae, dtype=instance.cfg.dtype, device=instance.cfg.device\n            )\n        instance.load_state_dict(state_dict[\"state_dict\"], strict=True)\n\n        return instance\n\n    @classmethod\n    def load_from_pretrained(cls, path: str, device: str = \"cpu\"):\n\n        config_path = os.path.join(path, \"cfg.json\")\n        weight_path = os.path.join(path, \"sae_weights.safetensors\")\n\n        with open(config_path, \"r\") as f:\n            config = json.load(f)\n\n        var_names = LanguageModelSAERunnerConfig.__init__.__code__.co_varnames\n        # filter config for varnames\n        config = {k: v for k, v in config.items() if k in var_names}\n        config[\"verbose\"] = False\n        config[\"device\"] = device\n        config = LanguageModelSAERunnerConfig(**config)\n        sae = SparseAutoencoder(config)\n\n        tensors = {}\n        with safe_open(weight_path, framework=\"pt\", device=device) as f:  # type: ignore\n            for k in f.keys():\n                tensors[k] = f.get_tensor(k)\n\n        # old saves may not have scaling factors.\n        if \"scaling_factor\" not in tensors:\n            assert isinstance(config.d_sae, int)\n            tensors[\"scaling_factor\"] = torch.ones(\n                config.d_sae, dtype=config.dtype, device=config.device\n            )\n\n        sae.load_state_dict(tensors)\n\n        return sae\n\n    def get_name(self):\n        sae_name = f\"sparse_autoencoder_{self.cfg.model_name}_{self.cfg.hook_point}_{self.cfg.d_sae}\"\n        return sae_name\n\n    def calculate_ghost_grad_loss(\n        self,\n        x: torch.Tensor,\n        sae_out: torch.Tensor,\n        per_item_mse_loss: torch.Tensor,\n        hidden_pre: torch.Tensor,\n        dead_neuron_mask: torch.Tensor,\n    ) -&gt; torch.Tensor:\n        # 1.\n        residual = x - sae_out\n        l2_norm_residual = torch.norm(residual, dim=-1)\n\n        # 2.\n        feature_acts_dead_neurons_only = torch.exp(hidden_pre[:, dead_neuron_mask])\n        ghost_out = feature_acts_dead_neurons_only @ self.W_dec[dead_neuron_mask, :]\n        l2_norm_ghost_out = torch.norm(ghost_out, dim=-1)\n        norm_scaling_factor = l2_norm_residual / (1e-6 + l2_norm_ghost_out * 2)\n        ghost_out = ghost_out * norm_scaling_factor[:, None].detach()\n\n        # 3.\n        per_item_mse_loss_ghost_resid = _per_item_mse_loss_with_target_norm(\n            ghost_out, residual.detach(), self.cfg.mse_loss_normalization\n        )\n        mse_rescaling_factor = (\n            per_item_mse_loss / (per_item_mse_loss_ghost_resid + 1e-6)\n        ).detach()\n        per_item_mse_loss_ghost_resid = (\n            mse_rescaling_factor * per_item_mse_loss_ghost_resid\n        )\n\n        return per_item_mse_loss_ghost_resid.mean()\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.load_from_pretrained_legacy","title":"<code>load_from_pretrained_legacy(path)</code>  <code>classmethod</code>","text":"<p>Load function for the model. Loads the model's state_dict and the config used to train it. This method can be called directly on the class, without needing an instance.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>@classmethod\ndef load_from_pretrained_legacy(cls, path: str):\n    \"\"\"\n    Load function for the model. Loads the model's state_dict and the config used to train it.\n    This method can be called directly on the class, without needing an instance.\n    \"\"\"\n\n    # Ensure the file exists\n    if not os.path.isfile(path):\n        raise FileNotFoundError(f\"No file found at specified path: {path}\")\n\n    # Load the state dictionary\n    if path.endswith(\".pt\"):\n        try:\n            if torch.backends.mps.is_available():\n                state_dict = torch.load(\n                    path,\n                    map_location=\"mps\",\n                    pickle_module=BackwardsCompatiblePickleClass,\n                )\n                state_dict[\"cfg\"].device = \"mps\"\n            else:\n                state_dict = torch.load(\n                    path, pickle_module=BackwardsCompatiblePickleClass\n                )\n        except Exception as e:\n            raise IOError(f\"Error loading the state dictionary from .pt file: {e}\")\n    elif path.endswith(\".pkl.gz\"):\n        try:\n            with gzip.open(path, \"rb\") as f:\n                state_dict = pickle.load(f)\n        except Exception as e:\n            raise IOError(\n                f\"Error loading the state dictionary from .pkl.gz file: {e}\"\n            )\n    elif path.endswith(\".pkl\"):\n        try:\n            with open(path, \"rb\") as f:\n                state_dict = pickle.load(f)\n        except Exception as e:\n            raise IOError(f\"Error loading the state dictionary from .pkl file: {e}\")\n    else:\n        raise ValueError(\n            f\"Unexpected file extension: {path}, supported extensions are .pt, .pkl, and .pkl.gz\"\n        )\n\n    # Ensure the loaded state contains both 'cfg' and 'state_dict'\n    if \"cfg\" not in state_dict or \"state_dict\" not in state_dict:\n        raise ValueError(\n            \"The loaded state dictionary must contain 'cfg' and 'state_dict' keys\"\n        )\n\n    # Create an instance of the class using the loaded configuration\n    instance = cls(cfg=state_dict[\"cfg\"])\n    if \"scaling_factor\" not in state_dict[\"state_dict\"]:\n        assert isinstance(instance.cfg.d_sae, int)\n        state_dict[\"state_dict\"][\"scaling_factor\"] = torch.ones(\n            instance.cfg.d_sae, dtype=instance.cfg.dtype, device=instance.cfg.device\n        )\n    instance.load_state_dict(state_dict[\"state_dict\"], strict=True)\n\n    return instance\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.remove_gradient_parallel_to_decoder_directions","title":"<code>remove_gradient_parallel_to_decoder_directions()</code>","text":"<p>Update grads so that they remove the parallel component     (d_sae, d_in) shape</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>@torch.no_grad()\ndef remove_gradient_parallel_to_decoder_directions(self):\n    \"\"\"\n    Update grads so that they remove the parallel component\n        (d_sae, d_in) shape\n    \"\"\"\n    assert self.W_dec.grad is not None  # keep pyright happy\n\n    parallel_component = einops.einsum(\n        self.W_dec.grad,\n        self.W_dec.data,\n        \"d_sae d_in, d_sae d_in -&gt; d_sae\",\n    )\n    self.W_dec.grad -= einops.einsum(\n        parallel_component,\n        self.W_dec.data,\n        \"d_sae, d_sae d_in -&gt; d_sae d_in\",\n    )\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.sparse_autoencoder.SparseAutoencoder.save_model_legacy","title":"<code>save_model_legacy(path)</code>","text":"<p>Basic save function for the model. Saves the model's state_dict and the config used to train it.</p> Source code in <code>sae_lens/training/sparse_autoencoder.py</code> <pre><code>def save_model_legacy(self, path: str):\n    \"\"\"\n    Basic save function for the model. Saves the model's state_dict and the config used to train it.\n    \"\"\"\n\n    # check if path exists\n    folder = os.path.dirname(path)\n    os.makedirs(folder, exist_ok=True)\n\n    state_dict = {\"cfg\": self.cfg, \"state_dict\": self.state_dict()}\n\n    if path.endswith(\".pt\"):\n        torch.save(state_dict, path)\n    else:\n        raise ValueError(\n            f\"Unexpected file extension: {path}, supported extensions are .pt and .pkl.gz\"\n        )\n\n    print(f\"Saved model to {path}\")\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore","title":"<code>ActivationsStore</code>","text":"<p>Class for streaming tokens and generating and storing activations while training SAEs.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>class ActivationsStore:\n    \"\"\"\n    Class for streaming tokens and generating and storing activations\n    while training SAEs.\n    \"\"\"\n\n    model: HookedRootModule\n    dataset: HfDataset\n    cached_activations_path: str | None\n    tokens_column: Literal[\"tokens\", \"input_ids\", \"text\"]\n    hook_point_head_index: int | None\n    _dataloader: Iterator[Any] | None = None\n    _storage_buffer: torch.Tensor | None = None\n\n    @classmethod\n    def from_config(\n        cls,\n        model: HookedRootModule,\n        cfg: LanguageModelSAERunnerConfig | CacheActivationsRunnerConfig,\n        dataset: HfDataset | None = None,\n    ) -&gt; \"ActivationsStore\":\n        cached_activations_path = cfg.cached_activations_path\n        # set cached_activations_path to None if we're not using cached activations\n        if (\n            isinstance(cfg, LanguageModelSAERunnerConfig)\n            and not cfg.use_cached_activations\n        ):\n            cached_activations_path = None\n        return cls(\n            model=model,\n            dataset=dataset or cfg.dataset_path,\n            hook_point=cfg.hook_point,\n            hook_point_layers=listify(cfg.hook_point_layer),\n            hook_point_head_index=cfg.hook_point_head_index,\n            context_size=cfg.context_size,\n            d_in=cfg.d_in,\n            n_batches_in_buffer=cfg.n_batches_in_buffer,\n            total_training_tokens=cfg.training_tokens,\n            store_batch_size=cfg.store_batch_size,\n            train_batch_size=cfg.train_batch_size,\n            prepend_bos=cfg.prepend_bos,\n            device=cfg.device,\n            dtype=cfg.dtype,\n            cached_activations_path=cached_activations_path,\n            model_kwargs=cfg.model_kwargs,\n        )\n\n    def __init__(\n        self,\n        model: HookedRootModule,\n        dataset: HfDataset | str,\n        hook_point: str,\n        hook_point_layers: list[int],\n        hook_point_head_index: int | None,\n        context_size: int,\n        d_in: int,\n        n_batches_in_buffer: int,\n        total_training_tokens: int,\n        store_batch_size: int,\n        train_batch_size: int,\n        prepend_bos: bool,\n        device: str | torch.device,\n        dtype: str | torch.dtype,\n        cached_activations_path: str | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n    ):\n        self.model = model\n        if model_kwargs is None:\n            model_kwargs = {}\n        self.model_kwargs = model_kwargs\n        self.dataset = (\n            load_dataset(dataset, split=\"train\", streaming=True)\n            if isinstance(dataset, str)\n            else dataset\n        )\n        self.hook_point = hook_point\n        self.hook_point_layers = hook_point_layers\n        self.hook_point_head_index = hook_point_head_index\n        self.context_size = context_size\n        self.d_in = d_in\n        self.n_batches_in_buffer = n_batches_in_buffer\n        self.total_training_tokens = total_training_tokens\n        self.store_batch_size = store_batch_size\n        self.train_batch_size = train_batch_size\n        self.prepend_bos = prepend_bos\n        self.device = device\n        self.dtype = dtype\n        self.cached_activations_path = cached_activations_path\n\n        self.iterable_dataset = iter(self.dataset)\n\n        # Check if dataset is tokenized\n        dataset_sample = next(self.iterable_dataset)\n\n        # check if it's tokenized\n        if \"tokens\" in dataset_sample.keys():\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"tokens\"\n        elif \"input_ids\" in dataset_sample.keys():\n            self.is_dataset_tokenized = True\n            self.tokens_column = \"input_ids\"\n        elif \"text\" in dataset_sample.keys():\n            self.is_dataset_tokenized = False\n            self.tokens_column = \"text\"\n        else:\n            raise ValueError(\n                \"Dataset must have a 'tokens', 'input_ids', or 'text' column.\"\n            )\n        self.iterable_dataset = iter(self.dataset)  # Reset iterator after checking\n\n        if cached_activations_path is not None:  # EDIT: load from multi-layer acts\n            assert self.cached_activations_path is not None  # keep pyright happy\n            # Sanity check: does the cache directory exist?\n            assert os.path.exists(\n                self.cached_activations_path\n            ), f\"Cache directory {self.cached_activations_path} does not exist. Consider double-checking your dataset, model, and hook names.\"\n\n            self.next_cache_idx = 0  # which file to open next\n            self.next_idx_within_buffer = 0  # where to start reading from in that file\n\n            # Check that we have enough data on disk\n            first_buffer = torch.load(f\"{self.cached_activations_path}/0.pt\")\n            buffer_size_on_disk = first_buffer.shape[0]\n            n_buffers_on_disk = len(os.listdir(self.cached_activations_path))\n            # Note: we're assuming all files have the same number of tokens\n            # (which seems reasonable imo since that's what our script does)\n            n_activations_on_disk = buffer_size_on_disk * n_buffers_on_disk\n            assert (\n                n_activations_on_disk &gt; self.total_training_tokens\n            ), f\"Only {n_activations_on_disk/1e6:.1f}M activations on disk, but total_training_tokens is {self.total_training_tokens/1e6:.1f}M.\"\n\n            # TODO add support for \"mixed loading\" (ie use cache until you run out, then switch over to streaming from HF)\n\n    @property\n    def storage_buffer(self) -&gt; torch.Tensor:\n        if self._storage_buffer is None:\n            self._storage_buffer = self.get_buffer(self.n_batches_in_buffer // 2)\n        return self._storage_buffer\n\n    @property\n    def dataloader(self) -&gt; Iterator[Any]:\n        if self._dataloader is None:\n            self._dataloader = self.get_data_loader()\n        return self._dataloader\n\n    def get_batch_tokens(self):\n        \"\"\"\n        Streams a batch of tokens from a dataset.\n        \"\"\"\n\n        batch_size = self.store_batch_size\n        context_size = self.context_size\n        device = self.device\n\n        batch_tokens = torch.zeros(\n            size=(0, context_size), device=device, dtype=torch.long, requires_grad=False\n        )\n\n        current_batch = []\n        current_length = 0\n\n        # pbar = tqdm(total=batch_size, desc=\"Filling batches\")\n        while batch_tokens.shape[0] &lt; batch_size:\n            tokens = self._get_next_dataset_tokens()\n            token_len = tokens.shape[0]\n\n            # TODO: Fix this so that we are limiting how many tokens we get from the same context.\n            assert self.model.tokenizer is not None  # keep pyright happy\n            while token_len &gt; 0 and batch_tokens.shape[0] &lt; batch_size:\n                # Space left in the current batch\n                space_left = context_size - current_length\n\n                # If the current tokens fit entirely into the remaining space\n                if token_len &lt;= space_left:\n                    current_batch.append(tokens[:token_len])\n                    current_length += token_len\n                    break\n\n                else:\n                    # Take as much as will fit\n                    current_batch.append(tokens[:space_left])\n\n                    # Remove used part, add BOS\n                    tokens = tokens[space_left:]\n                    token_len -= space_left\n\n                    # only add BOS if it's not already the first token\n                    if self.prepend_bos:\n                        bos_token_id_tensor = torch.tensor(\n                            [self.model.tokenizer.bos_token_id],\n                            device=tokens.device,\n                            dtype=torch.long,\n                        )\n                        if tokens[0] != bos_token_id_tensor:\n                            tokens = torch.cat(\n                                (\n                                    bos_token_id_tensor,\n                                    tokens,\n                                ),\n                                dim=0,\n                            )\n                            token_len += 1\n                    current_length = context_size\n\n                # If a batch is full, concatenate and move to next batch\n                if current_length == context_size:\n                    full_batch = torch.cat(current_batch, dim=0)\n                    batch_tokens = torch.cat(\n                        (batch_tokens, full_batch.unsqueeze(0)), dim=0\n                    )\n                    current_batch = []\n                    current_length = 0\n\n            # pbar.n = batch_tokens.shape[0]\n            # pbar.refresh()\n        return batch_tokens[:batch_size]\n\n    def get_activations(self, batch_tokens: torch.Tensor):\n        \"\"\"\n        Returns activations of shape (batches, context, num_layers, d_in)\n\n        d_in may result from a concatenated head dimension.\n        \"\"\"\n        layers = self.hook_point_layers\n        act_names = [self.hook_point.format(layer=layer) for layer in layers]\n        hook_point_max_layer = max(layers)\n        layerwise_activations = self.model.run_with_cache(\n            batch_tokens,\n            names_filter=act_names,\n            stop_at_layer=hook_point_max_layer + 1,\n            prepend_bos=self.prepend_bos,\n            **self.model_kwargs,\n        )[1]\n        activations_list = [layerwise_activations[act_name] for act_name in act_names]\n        if self.hook_point_head_index is not None:\n            activations_list = [\n                act[:, :, self.hook_point_head_index] for act in activations_list\n            ]\n        elif activations_list[0].ndim &gt; 3:  # if we have a head dimension\n            # flatten the head dimension\n            activations_list = [\n                act.view(act.shape[0], act.shape[1], -1) for act in activations_list\n            ]\n\n        # Stack along a new dimension to keep separate layers distinct\n        stacked_activations = torch.stack(activations_list, dim=2)\n\n        return stacked_activations\n\n    def get_buffer(self, n_batches_in_buffer: int) -&gt; torch.Tensor:\n        context_size = self.context_size\n        batch_size = self.store_batch_size\n        d_in = self.d_in\n        total_size = batch_size * n_batches_in_buffer\n        num_layers = len(self.hook_point_layers)  # Number of hook points or layers\n\n        if self.cached_activations_path is not None:\n            # Load the activations from disk\n            buffer_size = total_size * context_size\n            # Initialize an empty tensor with an additional dimension for layers\n            new_buffer = torch.zeros(\n                (buffer_size, num_layers, d_in),\n                dtype=self.dtype,  # type: ignore\n                device=self.device,\n            )\n            n_tokens_filled = 0\n\n            # Assume activations for different layers are stored separately and need to be combined\n            while n_tokens_filled &lt; buffer_size:\n                if not os.path.exists(\n                    f\"{self.cached_activations_path}/{self.next_cache_idx}.pt\"\n                ):\n                    print(\n                        \"\\n\\nWarning: Ran out of cached activation files earlier than expected.\"\n                    )\n                    print(\n                        f\"Expected to have {buffer_size} activations, but only found {n_tokens_filled}.\"\n                    )\n                    if buffer_size % self.total_training_tokens != 0:\n                        print(\n                            \"This might just be a rounding error \u2014 your batch_size * n_batches_in_buffer * context_size is not divisible by your total_training_tokens\"\n                        )\n                    print(f\"Returning a buffer of size {n_tokens_filled} instead.\")\n                    print(\"\\n\\n\")\n                    new_buffer = new_buffer[:n_tokens_filled, ...]\n                    return new_buffer\n\n                activations = torch.load(\n                    f\"{self.cached_activations_path}/{self.next_cache_idx}.pt\"\n                )\n                taking_subset_of_file = False\n                if n_tokens_filled + activations.shape[0] &gt; buffer_size:\n                    activations = activations[: buffer_size - n_tokens_filled, ...]\n                    taking_subset_of_file = True\n\n                new_buffer[\n                    n_tokens_filled : n_tokens_filled + activations.shape[0], ...\n                ] = activations\n\n                if taking_subset_of_file:\n                    self.next_idx_within_buffer = activations.shape[0]\n                else:\n                    self.next_cache_idx += 1\n                    self.next_idx_within_buffer = 0\n\n                n_tokens_filled += activations.shape[0]\n\n            return new_buffer\n\n        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)\n        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n        new_buffer = torch.zeros(\n            (total_size, context_size, num_layers, d_in),\n            dtype=self.dtype,  # type: ignore\n            device=self.device,\n        )\n\n        for refill_batch_idx_start in refill_iterator:\n            refill_batch_tokens = self.get_batch_tokens()\n            refill_activations = self.get_activations(refill_batch_tokens)\n            new_buffer[\n                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n            ] = refill_activations\n\n            # pbar.update(1)\n\n        new_buffer = new_buffer.reshape(-1, num_layers, d_in)\n        new_buffer = new_buffer[torch.randperm(new_buffer.shape[0])]\n\n        return new_buffer\n\n    def get_data_loader(\n        self,\n    ) -&gt; Iterator[Any]:\n        \"\"\"\n        Return a torch.utils.dataloader which you can get batches from.\n\n        Should automatically refill the buffer when it gets to n % full.\n        (better mixing if you refill and shuffle regularly).\n\n        \"\"\"\n\n        batch_size = self.train_batch_size\n\n        # 1. # create new buffer by mixing stored and new buffer\n        mixing_buffer = torch.cat(\n            [self.get_buffer(self.n_batches_in_buffer // 2), self.storage_buffer],\n            dim=0,\n        )\n\n        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n\n        # 2.  put 50 % in storage\n        self._storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n\n        # 3. put other 50 % in a dataloader\n        dataloader = iter(\n            DataLoader(\n                # TODO: seems like a typing bug?\n                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n                batch_size=batch_size,\n                shuffle=True,\n            )\n        )\n\n        return dataloader\n\n    def next_batch(self):\n        \"\"\"\n        Get the next batch from the current DataLoader.\n        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n        \"\"\"\n        try:\n            # Try to get the next batch\n            return next(self.dataloader)\n        except StopIteration:\n            # If the DataLoader is exhausted, create a new one\n            self._dataloader = self.get_data_loader()\n            return next(self.dataloader)\n\n    def _get_next_dataset_tokens(self) -&gt; torch.Tensor:\n        device = self.device\n        if not self.is_dataset_tokenized:\n            s = next(self.iterable_dataset)[self.tokens_column]\n            tokens = self.model.to_tokens(\n                s,\n                truncate=True,\n                move_to_device=True,\n                prepend_bos=self.prepend_bos,\n            ).squeeze(0)\n            assert (\n                len(tokens.shape) == 1\n            ), f\"tokens.shape should be 1D but was {tokens.shape}\"\n        else:\n            tokens = torch.tensor(\n                next(self.iterable_dataset)[self.tokens_column],\n                dtype=torch.long,\n                device=device,\n                requires_grad=False,\n            )\n            if (\n                not self.prepend_bos\n                and tokens[0] == self.model.tokenizer.bos_token_id  # type: ignore\n            ):\n                tokens = tokens[1:]\n        return tokens\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.get_activations","title":"<code>get_activations(batch_tokens)</code>","text":"<p>Returns activations of shape (batches, context, num_layers, d_in)</p> <p>d_in may result from a concatenated head dimension.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_activations(self, batch_tokens: torch.Tensor):\n    \"\"\"\n    Returns activations of shape (batches, context, num_layers, d_in)\n\n    d_in may result from a concatenated head dimension.\n    \"\"\"\n    layers = self.hook_point_layers\n    act_names = [self.hook_point.format(layer=layer) for layer in layers]\n    hook_point_max_layer = max(layers)\n    layerwise_activations = self.model.run_with_cache(\n        batch_tokens,\n        names_filter=act_names,\n        stop_at_layer=hook_point_max_layer + 1,\n        prepend_bos=self.prepend_bos,\n        **self.model_kwargs,\n    )[1]\n    activations_list = [layerwise_activations[act_name] for act_name in act_names]\n    if self.hook_point_head_index is not None:\n        activations_list = [\n            act[:, :, self.hook_point_head_index] for act in activations_list\n        ]\n    elif activations_list[0].ndim &gt; 3:  # if we have a head dimension\n        # flatten the head dimension\n        activations_list = [\n            act.view(act.shape[0], act.shape[1], -1) for act in activations_list\n        ]\n\n    # Stack along a new dimension to keep separate layers distinct\n    stacked_activations = torch.stack(activations_list, dim=2)\n\n    return stacked_activations\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.get_batch_tokens","title":"<code>get_batch_tokens()</code>","text":"<p>Streams a batch of tokens from a dataset.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_batch_tokens(self):\n    \"\"\"\n    Streams a batch of tokens from a dataset.\n    \"\"\"\n\n    batch_size = self.store_batch_size\n    context_size = self.context_size\n    device = self.device\n\n    batch_tokens = torch.zeros(\n        size=(0, context_size), device=device, dtype=torch.long, requires_grad=False\n    )\n\n    current_batch = []\n    current_length = 0\n\n    # pbar = tqdm(total=batch_size, desc=\"Filling batches\")\n    while batch_tokens.shape[0] &lt; batch_size:\n        tokens = self._get_next_dataset_tokens()\n        token_len = tokens.shape[0]\n\n        # TODO: Fix this so that we are limiting how many tokens we get from the same context.\n        assert self.model.tokenizer is not None  # keep pyright happy\n        while token_len &gt; 0 and batch_tokens.shape[0] &lt; batch_size:\n            # Space left in the current batch\n            space_left = context_size - current_length\n\n            # If the current tokens fit entirely into the remaining space\n            if token_len &lt;= space_left:\n                current_batch.append(tokens[:token_len])\n                current_length += token_len\n                break\n\n            else:\n                # Take as much as will fit\n                current_batch.append(tokens[:space_left])\n\n                # Remove used part, add BOS\n                tokens = tokens[space_left:]\n                token_len -= space_left\n\n                # only add BOS if it's not already the first token\n                if self.prepend_bos:\n                    bos_token_id_tensor = torch.tensor(\n                        [self.model.tokenizer.bos_token_id],\n                        device=tokens.device,\n                        dtype=torch.long,\n                    )\n                    if tokens[0] != bos_token_id_tensor:\n                        tokens = torch.cat(\n                            (\n                                bos_token_id_tensor,\n                                tokens,\n                            ),\n                            dim=0,\n                        )\n                        token_len += 1\n                current_length = context_size\n\n            # If a batch is full, concatenate and move to next batch\n            if current_length == context_size:\n                full_batch = torch.cat(current_batch, dim=0)\n                batch_tokens = torch.cat(\n                    (batch_tokens, full_batch.unsqueeze(0)), dim=0\n                )\n                current_batch = []\n                current_length = 0\n\n        # pbar.n = batch_tokens.shape[0]\n        # pbar.refresh()\n    return batch_tokens[:batch_size]\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.get_data_loader","title":"<code>get_data_loader()</code>","text":"<p>Return a torch.utils.dataloader which you can get batches from.</p> <p>Should automatically refill the buffer when it gets to n % full. (better mixing if you refill and shuffle regularly).</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def get_data_loader(\n    self,\n) -&gt; Iterator[Any]:\n    \"\"\"\n    Return a torch.utils.dataloader which you can get batches from.\n\n    Should automatically refill the buffer when it gets to n % full.\n    (better mixing if you refill and shuffle regularly).\n\n    \"\"\"\n\n    batch_size = self.train_batch_size\n\n    # 1. # create new buffer by mixing stored and new buffer\n    mixing_buffer = torch.cat(\n        [self.get_buffer(self.n_batches_in_buffer // 2), self.storage_buffer],\n        dim=0,\n    )\n\n    mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n\n    # 2.  put 50 % in storage\n    self._storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n\n    # 3. put other 50 % in a dataloader\n    dataloader = iter(\n        DataLoader(\n            # TODO: seems like a typing bug?\n            cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n            batch_size=batch_size,\n            shuffle=True,\n        )\n    )\n\n    return dataloader\n</code></pre>"},{"location":"reference/language_models/#sae_lens.training.activations_store.ActivationsStore.next_batch","title":"<code>next_batch()</code>","text":"<p>Get the next batch from the current DataLoader. If the DataLoader is exhausted, refill the buffer and create a new DataLoader.</p> Source code in <code>sae_lens/training/activations_store.py</code> <pre><code>def next_batch(self):\n    \"\"\"\n    Get the next batch from the current DataLoader.\n    If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n    \"\"\"\n    try:\n        # Try to get the next batch\n        return next(self.dataloader)\n    except StopIteration:\n        # If the DataLoader is exhausted, create a new one\n        self._dataloader = self.get_data_loader()\n        return next(self.dataloader)\n</code></pre>"},{"location":"reference/misc/","title":"Misc","text":"<p>Took the LR scheduler from my previous work: https://github.com/jbloomAus/DecisionTransformerInterpretability/blob/ee55df35cdb92e81d689c72fb9dd5a7252893363/src/decision_transformer/utils.py#L425</p>"},{"location":"reference/misc/#sae_lens.training.config.CacheActivationsRunnerConfig","title":"<code>CacheActivationsRunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for caching activations of an LLM.</p> Source code in <code>sae_lens/training/config.py</code> <pre><code>@dataclass\nclass CacheActivationsRunnerConfig:\n    \"\"\"\n    Configuration for caching activations of an LLM.\n    \"\"\"\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name: str = \"gelu-2l\"\n    model_class_name: str = \"HookedTransformer\"\n    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n    hook_point_layer: int | list[int] = 0\n    hook_point_head_index: Optional[int] = None\n    dataset_path: str = \"NeelNanda/c4-tokenized-2b\"\n    is_dataset_tokenized: bool = True\n    context_size: int = 128\n    cached_activations_path: Optional[str] = (\n        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_point_head_index}\"\n    )\n\n    # SAE Parameters\n    d_in: int = 512\n\n    # Activation Store Parameters\n    n_batches_in_buffer: int = 20\n    training_tokens: int = 2_000_000\n    store_batch_size: int = 32\n    train_batch_size: int = 4096\n\n    # Misc\n    device: str | torch.device = \"cpu\"\n    seed: int = 42\n    dtype: str | torch.dtype = \"float32\"\n    prepend_bos: bool = True\n\n    # Activation caching stuff\n    shuffle_every_n_buffers: int = 10\n    n_shuffles_with_last_section: int = 10\n    n_shuffles_in_entire_dir: int = 10\n    n_shuffles_final: int = 100\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        # Autofill cached_activations_path unless the user overrode it\n        if self.cached_activations_path is None:\n            self.cached_activations_path = _default_cached_activations_path(\n                self.dataset_path,\n                self.model_name,\n                self.hook_point,\n                self.hook_point_head_index,\n            )\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.config.LanguageModelSAERunnerConfig","title":"<code>LanguageModelSAERunnerConfig</code>  <code>dataclass</code>","text":"<p>Configuration for training a sparse autoencoder on a language model.</p> Source code in <code>sae_lens/training/config.py</code> <pre><code>@dataclass\nclass LanguageModelSAERunnerConfig:\n    \"\"\"\n    Configuration for training a sparse autoencoder on a language model.\n    \"\"\"\n\n    # Data Generating Function (Model + Training Distibuion)\n    model_name: str = \"gelu-2l\"\n    model_class_name: str = \"HookedTransformer\"\n    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n    hook_point_eval: str = \"blocks.{layer}.attn.pattern\"\n    hook_point_layer: int | list[int] = 0\n    hook_point_head_index: Optional[int] = None\n    dataset_path: str = \"NeelNanda/c4-tokenized-2b\"\n    is_dataset_tokenized: bool = True\n    context_size: int = 128\n    use_cached_activations: bool = False\n    cached_activations_path: Optional[str] = (\n        None  # Defaults to \"activations/{dataset}/{model}/{full_hook_name}_{hook_point_head_index}\"\n    )\n\n    # SAE Parameters\n    d_in: int = 512\n    d_sae: Optional[int] = None\n    b_dec_init_method: str = \"geometric_median\"\n    expansion_factor: int | list[int] = 4\n    activation_fn: str = \"relu\"  # relu, tanh-relu\n    normalize_sae_decoder: bool = True\n    noise_scale: float = 0.0\n    from_pretrained_path: Optional[str] = None\n    apply_b_dec_to_input: bool = True\n    decoder_orthogonal_init: bool = False\n\n    # Activation Store Parameters\n    n_batches_in_buffer: int = 20\n    training_tokens: int = 2_000_000\n    finetuning_tokens: int = 0\n    store_batch_size: int = 32\n    train_batch_size: int = 4096\n\n    # Misc\n    device: str | torch.device = \"cpu\"\n    seed: int = 42\n    dtype: str | torch.dtype = \"float32\"  # type: ignore #\n    prepend_bos: bool = True\n\n    # Training Parameters\n\n    ## Batch size\n    train_batch_size: int = 4096\n\n    ## Adam\n    adam_beta1: float | list[float] = 0\n    adam_beta2: float | list[float] = 0.999\n\n    ## Loss Function\n    mse_loss_normalization: Optional[str] = None\n    l1_coefficient: float | list[float] = 1e-3\n    lp_norm: float | list[float] = 1\n\n    ## Learning Rate Schedule\n    lr: float | list[float] = 3e-4\n    lr_scheduler_name: str | list[str] = (\n        \"constant\"  # constant, cosineannealing, cosineannealingwarmrestarts\n    )\n    lr_warm_up_steps: int | list[int] = 500\n    lr_end: float | list[float] | None = (\n        None  # only used for cosine annealing, default is lr / 10\n    )\n    lr_decay_steps: int | list[int] = 0\n    n_restart_cycles: int | list[int] = 1  # used only for cosineannealingwarmrestarts\n\n    ## FineTuning\n    finetuning_method: Optional[str] = None  # scale, decoder or unrotated_decoder\n\n    # Resampling protocol args\n    use_ghost_grads: bool | list[bool] = (\n        False  # want to change this to true on some timeline.\n    )\n    feature_sampling_window: int = 2000\n    dead_feature_window: int = 1000  # unless this window is larger feature sampling,\n\n    dead_feature_threshold: float = 1e-8\n\n    # WANDB\n    log_to_wandb: bool = True\n    wandb_project: str = \"mats_sae_training_language_model\"\n    run_name: Optional[str] = None\n    wandb_entity: Optional[str] = None\n    wandb_log_frequency: int = 10\n\n    # Misc\n    n_checkpoints: int = 0\n    checkpoint_path: str = \"checkpoints\"\n    verbose: bool = True\n    model_kwargs: dict[str, Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        if self.use_cached_activations and self.cached_activations_path is None:\n            self.cached_activations_path = _default_cached_activations_path(\n                self.dataset_path,\n                self.model_name,\n                self.hook_point,\n                self.hook_point_head_index,\n            )\n\n        if not isinstance(self.expansion_factor, list):\n            self.d_sae = self.d_in * self.expansion_factor\n        self.tokens_per_buffer = (\n            self.train_batch_size * self.context_size * self.n_batches_in_buffer\n        )\n\n        if self.run_name is None:\n            self.run_name = f\"{self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n\n        if self.b_dec_init_method not in [\"geometric_median\", \"mean\", \"zeros\"]:\n            raise ValueError(\n                f\"b_dec_init_method must be geometric_median, mean, or zeros. Got {self.b_dec_init_method}\"\n            )\n        if self.b_dec_init_method == \"zeros\":\n            print(\n                \"Warning: We are initializing b_dec to zeros. This is probably not what you want.\"\n            )\n\n        if isinstance(self.dtype, str) and self.dtype not in DTYPE_MAP:\n            raise ValueError(\n                f\"dtype must be one of {list(DTYPE_MAP.keys())}. Got {self.dtype}\"\n            )\n        elif isinstance(self.dtype, str):\n            self.dtype: torch.dtype = DTYPE_MAP[self.dtype]\n\n        # if we use decoder fine tuning, we can't be applying b_dec to the input\n        if (self.finetuning_method == \"decoder\") and (self.apply_b_dec_to_input):\n            raise ValueError(\n                \"If we are fine tuning the decoder, we can't be applying b_dec to the input.\\nSet apply_b_dec_to_input to False.\"\n            )\n\n        self.device: str | torch.device = torch.device(self.device)\n\n        if self.lr_end is None:\n            if isinstance(self.lr, list):\n                self.lr_end = [lr / 10 for lr in self.lr]\n            else:\n                self.lr_end = self.lr / 10\n\n        unique_id = cast(\n            Any, wandb\n        ).util.generate_id()  # not sure why this type is erroring\n        self.checkpoint_path = f\"{self.checkpoint_path}/{unique_id}\"\n\n        if self.verbose:\n            print(\n                f\"Run name: {self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}\"\n            )\n            # Print out some useful info:\n            n_tokens_per_buffer = (\n                self.store_batch_size * self.context_size * self.n_batches_in_buffer\n            )\n            print(f\"n_tokens_per_buffer (millions): {n_tokens_per_buffer / 10 ** 6}\")\n            n_contexts_per_buffer = self.store_batch_size * self.n_batches_in_buffer\n            print(\n                f\"Lower bound: n_contexts_per_buffer (millions): {n_contexts_per_buffer / 10 ** 6}\"\n            )\n\n            total_training_steps = (\n                self.training_tokens + self.finetuning_tokens\n            ) // self.train_batch_size\n            print(f\"Total training steps: {total_training_steps}\")\n\n            total_wandb_updates = total_training_steps // self.wandb_log_frequency\n            print(f\"Total wandb updates: {total_wandb_updates}\")\n\n            # how many times will we sample dead neurons?\n            # assert self.dead_feature_window &lt;= self.feature_sampling_window, \"dead_feature_window must be smaller than feature_sampling_window\"\n            n_feature_window_samples = (\n                total_training_steps // self.feature_sampling_window\n            )\n            print(\n                f\"n_tokens_per_feature_sampling_window (millions): {(self.feature_sampling_window * self.context_size * self.train_batch_size) / 10 ** 6}\"\n            )\n            print(\n                f\"n_tokens_per_dead_feature_window (millions): {(self.dead_feature_window * self.context_size * self.train_batch_size) / 10 ** 6}\"\n            )\n            print(\n                f\"We will reset the sparsity calculation {n_feature_window_samples} times.\"\n            )\n            # print(\"Number tokens in dead feature calculation window: \", self.dead_feature_window * self.train_batch_size)\n            print(\n                f\"Number tokens in sparsity calculation window: {self.feature_sampling_window * self.train_batch_size:.2e}\"\n            )\n\n        if not isinstance(self.use_ghost_grads, list) and self.use_ghost_grads:\n            print(\"Using Ghost Grads.\")\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader","title":"<code>LMSparseAutoencoderSessionloader</code>","text":"<p>Responsible for loading all required artifacts and files for training a sparse autoencoder on a language model or analysing a pretraining autoencoder</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>class LMSparseAutoencoderSessionloader:\n    \"\"\"\n    Responsible for loading all required\n    artifacts and files for training\n    a sparse autoencoder on a language model\n    or analysing a pretraining autoencoder\n    \"\"\"\n\n    def __init__(self, cfg: LanguageModelSAERunnerConfig):\n        self.cfg = cfg\n\n    def load_sae_training_group_session(\n        self,\n    ) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n        \"\"\"\n        Loads a session for training a sparse autoencoder on a language model.\n        \"\"\"\n\n        model = self.get_model(self.cfg.model_name)\n        model.to(self.cfg.device)\n        activations_loader = ActivationsStore.from_config(\n            model,\n            self.cfg,\n        )\n\n        sae_group = SparseAutoencoderDictionary(self.cfg)\n\n        return model, sae_group, activations_loader\n\n    @classmethod\n    def load_pretrained_sae(\n        cls, path: str, device: str = \"cpu\"\n    ) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n        \"\"\"\n        Loads a session for analysing a pretrained sparse autoencoder.\n        \"\"\"\n\n        # load the SAE\n        sparse_autoencoders = SparseAutoencoderDictionary.load_from_pretrained(\n            path, device\n        )\n        first_sparse_autoencoder_cfg = next(iter(sparse_autoencoders))[1].cfg\n\n        # load the model, SAE and activations loader with it.\n        session_loader = cls(first_sparse_autoencoder_cfg)\n        model, _, activations_loader = session_loader.load_sae_training_group_session()\n\n        return model, sparse_autoencoders, activations_loader\n\n    def get_model(self, model_name: str) -&gt; HookedRootModule:\n        \"\"\"\n        Loads a model from transformer lens.\n\n        Abstracted to allow for easy modification.\n        \"\"\"\n\n        # Todo: add check that model_name is valid\n\n        model = load_model(\n            self.cfg.model_class_name, model_name, device=self.cfg.device\n        )\n        return model\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader.get_model","title":"<code>get_model(model_name)</code>","text":"<p>Loads a model from transformer lens.</p> <p>Abstracted to allow for easy modification.</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>def get_model(self, model_name: str) -&gt; HookedRootModule:\n    \"\"\"\n    Loads a model from transformer lens.\n\n    Abstracted to allow for easy modification.\n    \"\"\"\n\n    # Todo: add check that model_name is valid\n\n    model = load_model(\n        self.cfg.model_class_name, model_name, device=self.cfg.device\n    )\n    return model\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader.load_pretrained_sae","title":"<code>load_pretrained_sae(path, device='cpu')</code>  <code>classmethod</code>","text":"<p>Loads a session for analysing a pretrained sparse autoencoder.</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>@classmethod\ndef load_pretrained_sae(\n    cls, path: str, device: str = \"cpu\"\n) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n    \"\"\"\n    Loads a session for analysing a pretrained sparse autoencoder.\n    \"\"\"\n\n    # load the SAE\n    sparse_autoencoders = SparseAutoencoderDictionary.load_from_pretrained(\n        path, device\n    )\n    first_sparse_autoencoder_cfg = next(iter(sparse_autoencoders))[1].cfg\n\n    # load the model, SAE and activations loader with it.\n    session_loader = cls(first_sparse_autoencoder_cfg)\n    model, _, activations_loader = session_loader.load_sae_training_group_session()\n\n    return model, sparse_autoencoders, activations_loader\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.session_loader.LMSparseAutoencoderSessionloader.load_sae_training_group_session","title":"<code>load_sae_training_group_session()</code>","text":"<p>Loads a session for training a sparse autoencoder on a language model.</p> Source code in <code>sae_lens/training/session_loader.py</code> <pre><code>def load_sae_training_group_session(\n    self,\n) -&gt; Tuple[HookedRootModule, SparseAutoencoderDictionary, ActivationsStore]:\n    \"\"\"\n    Loads a session for training a sparse autoencoder on a language model.\n    \"\"\"\n\n    model = self.get_model(self.cfg.model_name)\n    model.to(self.cfg.device)\n    activations_loader = ActivationsStore.from_config(\n        model,\n        self.cfg,\n    )\n\n    sae_group = SparseAutoencoderDictionary(self.cfg)\n\n    return model, sae_group, activations_loader\n</code></pre>"},{"location":"reference/misc/#sae_lens.training.optim.get_scheduler","title":"<code>get_scheduler(scheduler_name, optimizer, training_steps, lr, warm_up_steps, decay_steps, lr_end, num_cycles)</code>","text":"<p>Loosely based on this, seemed simpler write this than import transformers: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules</p> <p>Parameters:</p> Name Type Description Default <code>scheduler_name</code> <code>str</code> <p>Name of the scheduler to use, one of \"constant\", \"cosineannealing\", \"cosineannealingwarmrestarts\"</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer to use</p> required <code>training_steps</code> <code>int</code> <p>Total number of training steps</p> required <code>warm_up_steps</code> <code>int</code> <p>Number of linear warm up steps. Defaults to 0.</p> required <code>decay_steps</code> <code>int</code> <p>Number of linear decay steps to 0. Defaults to 0.</p> required <code>num_cycles</code> <code>int</code> <p>Number of cycles for cosine annealing with warm restarts. Defaults to 1.</p> required <code>lr_end</code> <code>float</code> <p>Final learning rate multiplier before decay. Defaults to 0.0.</p> required Source code in <code>sae_lens/training/optim.py</code> <pre><code>def get_scheduler(\n    scheduler_name: str,\n    optimizer: optim.Optimizer,\n    training_steps: int,\n    lr: float,\n    warm_up_steps: int,\n    decay_steps: int,\n    lr_end: float,\n    num_cycles: int,\n) -&gt; lr_scheduler.LRScheduler:\n    \"\"\"\n    Loosely based on this, seemed simpler write this than import\n    transformers: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules\n\n    Args:\n        scheduler_name (str): Name of the scheduler to use, one of \"constant\", \"cosineannealing\", \"cosineannealingwarmrestarts\"\n        optimizer (optim.Optimizer): Optimizer to use\n        training_steps (int): Total number of training steps\n        warm_up_steps (int, optional): Number of linear warm up steps. Defaults to 0.\n        decay_steps (int, optional): Number of linear decay steps to 0. Defaults to 0.\n        num_cycles (int, optional): Number of cycles for cosine annealing with warm restarts. Defaults to 1.\n        lr_end (float, optional): Final learning rate multiplier before decay. Defaults to 0.0.\n    \"\"\"\n    base_scheduler_steps = training_steps - warm_up_steps - decay_steps\n    norm_scheduler_name = scheduler_name.lower()\n    main_scheduler = _get_main_scheduler(\n        norm_scheduler_name,\n        optimizer,\n        steps=base_scheduler_steps,\n        lr_end=lr_end,\n        num_cycles=num_cycles,\n    )\n    if norm_scheduler_name == \"constant\":\n        # constant scheduler ignores lr_end, so decay needs to start at lr\n        lr_end = lr\n    schedulers: list[lr_scheduler.LRScheduler] = []\n    milestones: list[int] = []\n    if warm_up_steps &gt; 0:\n        schedulers.append(\n            lr_scheduler.LinearLR(\n                optimizer,\n                start_factor=1 / warm_up_steps,\n                end_factor=1.0,\n                total_iters=warm_up_steps - 1,\n            ),\n        )\n        milestones.append(warm_up_steps)\n    schedulers.append(main_scheduler)\n    if decay_steps &gt; 0:\n        if lr_end == 0.0:\n            raise ValueError(\n                \"Cannot have decay_steps with lr_end=0.0, this would decay from 0 to 0 and be a waste.\"\n            )\n        schedulers.append(\n            lr_scheduler.LinearLR(\n                optimizer,\n                start_factor=lr_end / lr,\n                end_factor=0.0,\n                total_iters=decay_steps,\n            ),\n        )\n        milestones.append(training_steps - decay_steps)\n    return lr_scheduler.SequentialLR(\n        schedulers=schedulers,\n        optimizer=optimizer,\n        milestones=milestones,\n    )\n</code></pre>"},{"location":"reference/runners/","title":"Runners","text":""},{"location":"reference/runners/#sae_lens.training.lm_runner.language_model_sae_runner","title":"<code>language_model_sae_runner(cfg)</code>","text":"Source code in <code>sae_lens/training/lm_runner.py</code> <pre><code>def language_model_sae_runner(cfg: LanguageModelSAERunnerConfig):\n    \"\"\" \"\"\"\n\n    if cfg.from_pretrained_path is not None:\n        (\n            model,\n            sparse_autoencoder,\n            activations_loader,\n        ) = LMSparseAutoencoderSessionloader.load_pretrained_sae(\n            cfg.from_pretrained_path\n        )\n        cfg = sparse_autoencoder.cfg\n    else:\n        loader = LMSparseAutoencoderSessionloader(cfg)\n        model, sparse_autoencoder, activations_loader = (\n            loader.load_sae_training_group_session()\n        )\n\n    if cfg.log_to_wandb:\n        wandb.init(project=cfg.wandb_project, config=cast(Any, cfg), name=cfg.run_name)\n\n    # train SAE\n    sparse_autoencoder = train_sae_on_language_model(\n        model,\n        sparse_autoencoder,\n        activations_loader,\n        n_checkpoints=cfg.n_checkpoints,\n        batch_size=cfg.train_batch_size,\n        feature_sampling_window=cfg.feature_sampling_window,\n        dead_feature_threshold=cfg.dead_feature_threshold,\n        use_wandb=cfg.log_to_wandb,\n        wandb_log_frequency=cfg.wandb_log_frequency,\n    )\n\n    if cfg.log_to_wandb:\n        wandb.finish()\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/toy_models/","title":"Toy Models","text":"<p>https://www.lesswrong.com/posts/LnHowHgmrMbWtpkxx/intro-to-superposition-and-sparse-autoencoders-colab?fbclid=IwAR04OCGu_unvxezvDWkys9_6MJPEnXuu6GSqU6ScrMkAb1bvdSYFOeS35AY https://github.com/callummcdougall/sae-exercises-mats?fbclid=IwAR3qYAELbyD_x5IAYN4yCDFQzxXHeuH6CwMi_E7g4Qg6G1QXRNAYabQ4xGs</p>"},{"location":"reference/toy_models/#sae_lens.training.train_sae_on_toy_model.train_toy_sae","title":"<code>train_toy_sae(sparse_autoencoder, activation_store, batch_size=1024, feature_sampling_window=100, dead_feature_window=2000, dead_feature_threshold=1e-08, use_wandb=False, wandb_log_frequency=50)</code>","text":"<p>Takes an SAE and a bunch of activations and does a bunch of training steps</p> Source code in <code>sae_lens/training/train_sae_on_toy_model.py</code> <pre><code>def train_toy_sae(\n    sparse_autoencoder: SparseAutoencoder,\n    activation_store: torch.Tensor,  # TODO: this type seems strange / wrong\n    batch_size: int = 1024,\n    feature_sampling_window: int = 100,  # how many training steps between resampling the features / considiring neurons dead\n    dead_feature_window: int = 2000,  # how many training steps before a feature is considered dead\n    dead_feature_threshold: float = 1e-8,  # how infrequently a feature has to be active to be considered dead\n    use_wandb: bool = False,\n    wandb_log_frequency: int = 50,\n):\n    \"\"\"\n    Takes an SAE and a bunch of activations and does a bunch of training steps\n    \"\"\"\n\n    # TODO: this type seems strange\n    dataloader = iter(\n        DataLoader(cast(Any, activation_store), batch_size=batch_size, shuffle=True)\n    )\n    optimizer = torch.optim.Adam(sparse_autoencoder.parameters())\n    sparse_autoencoder.train()\n    frac_active_list = []  # track active features\n\n    n_training_steps = 0\n    n_training_tokens = 0\n\n    pbar = tqdm(dataloader, desc=\"Training SAE\")\n    for _, batch in enumerate(pbar):\n        batch = next(dataloader)\n        # Make sure the W_dec is still zero-norm\n        if sparse_autoencoder.normalize_sae_decoder:\n            sparse_autoencoder.set_decoder_norm_to_unit_norm()\n\n        # Forward and Backward Passes\n        optimizer.zero_grad()\n        sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(batch)\n        loss.backward()\n        if sparse_autoencoder.normalize_sae_decoder:\n            sparse_autoencoder.remove_gradient_parallel_to_decoder_directions()\n        optimizer.step()\n\n        n_training_tokens += batch_size\n\n        with torch.no_grad():\n            # Calculate the sparsities, and add it to a list\n            act_freq_scores = (feature_acts.abs() &gt; 0).float().sum(0)\n            frac_active_list.append(act_freq_scores)\n\n            if len(frac_active_list) &gt; feature_sampling_window:\n                frac_active_in_window = torch.stack(\n                    frac_active_list[-feature_sampling_window:], dim=0\n                )\n                feature_sparsity = frac_active_in_window.sum(0) / (\n                    feature_sampling_window * batch_size\n                )\n            else:\n                # use the whole list\n                frac_active_in_window = torch.stack(frac_active_list, dim=0)\n                feature_sparsity = frac_active_in_window.sum(0) / (\n                    len(frac_active_list) * batch_size\n                )\n\n            l0 = (feature_acts &gt; 0).float().sum(-1).mean()\n            l2_norm = torch.norm(feature_acts, dim=1).mean()\n\n            l2_norm_in = torch.norm(batch, dim=-1)\n            l2_norm_out = torch.norm(sae_out, dim=-1)\n            l2_norm_ratio = l2_norm_out / (1e-6 + l2_norm_in)\n\n            if use_wandb and ((n_training_steps + 1) % wandb_log_frequency == 0):\n                wandb.log(\n                    {\n                        \"details/n_training_tokens\": n_training_tokens,\n                        \"losses/mse_loss\": mse_loss.item(),\n                        \"losses/l1_loss\": l1_loss.item(),\n                        \"losses/overall_loss\": loss.item(),\n                        \"metrics/l0\": l0.item(),\n                        \"metrics/l2\": l2_norm.item(),\n                        \"metrics/l2_ratio\": l2_norm_ratio.mean().item(),\n                        \"sparsity/below_1e-5\": (feature_sparsity &lt; 1e-5)\n                        .float()\n                        .mean()\n                        .item(),\n                        \"sparsity/below_1e-6\": (feature_sparsity &lt; 1e-6)\n                        .float()\n                        .mean()\n                        .item(),\n                        \"sparsity/n_dead_features\": (\n                            feature_sparsity &lt; dead_feature_threshold\n                        )\n                        .float()\n                        .mean()\n                        .item(),\n                    },\n                    step=n_training_steps,\n                )\n\n                if (n_training_steps + 1) % (wandb_log_frequency * 100) == 0:\n                    log_feature_sparsity = torch.log10(feature_sparsity + 1e-8)\n                    wandb.log(\n                        {\n                            \"plots/feature_density_histogram\": wandb.Histogram(\n                                log_feature_sparsity.tolist()\n                            ),\n                        },\n                        step=n_training_steps,\n                    )\n\n            pbar.set_description(\n                f\"{n_training_steps}| MSE Loss {mse_loss.item():.3f} | L0 {l0.item():.3f}\"\n            )\n            pbar.update(batch_size)\n\n        # If we did checkpointing we'd do it here.\n\n        n_training_steps += 1\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_model_runner.toy_model_sae_runner","title":"<code>toy_model_sae_runner(cfg)</code>","text":"<p>A runner for training an SAE on a toy model.</p> Source code in <code>sae_lens/training/toy_model_runner.py</code> <pre><code>def toy_model_sae_runner(cfg: SAEToyModelRunnerConfig):\n    \"\"\"\n    A runner for training an SAE on a toy model.\n    \"\"\"\n    # Toy Model Config\n    toy_model_cfg = ToyConfig(\n        n_instances=1,  # Not set up to train &gt; 1 SAE so shouldn't do &gt; 1 model.\n        n_features=cfg.n_features,\n        n_hidden=cfg.n_hidden,\n        n_correlated_pairs=cfg.n_correlated_pairs,\n        n_anticorrelated_pairs=cfg.n_anticorrelated_pairs,\n    )\n\n    # Initialize Toy Model\n    model = ToyModel(\n        cfg=toy_model_cfg,\n        device=cfg.device,\n        feature_probability=cfg.feature_probability,\n    )\n\n    # Train the Toy Model\n    model.optimize(steps=cfg.model_training_steps)\n\n    # Generate Training Data\n    batch = model.generate_batch(cfg.total_training_tokens)\n    hidden = einops.einsum(\n        batch,\n        model.W,\n        \"batch_size instances features, instances hidden features -&gt; batch_size instances hidden\",\n    )\n\n    sparse_autoencoder = SparseAutoencoder(\n        cast(Any, cfg)  # TODO: the types are broken here\n    )  # config has the hyperparameters for the SAE\n\n    if cfg.log_to_wandb:\n        wandb.init(project=cfg.wandb_project, config=cast(Any, cfg))\n\n    sparse_autoencoder = train_toy_sae(\n        sparse_autoencoder,\n        activation_store=hidden.detach().squeeze(),\n        batch_size=cfg.train_batch_size,\n        feature_sampling_window=cfg.feature_sampling_window,\n        dead_feature_threshold=cfg.dead_feature_threshold,\n        use_wandb=cfg.log_to_wandb,\n        wandb_log_frequency=cfg.wandb_log_frequency,\n    )\n\n    if cfg.log_to_wandb:\n        wandb.finish()\n\n    return sparse_autoencoder\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.Model","title":"<code>Model</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>class Model(nn.Module):\n    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n    b_final: Float[Tensor, \"n_instances n_features\"]\n    # Our linear map is x -&gt; ReLU(W.T @ W @ x + b_final)\n\n    def __init__(\n        self,\n        cfg: Config,\n        feature_probability: Optional[Union[float, Tensor]] = None,\n        importance: Optional[Union[float, Tensor]] = None,\n        device: str | torch.device = device,\n    ):\n        super().__init__()\n        self.cfg = cfg\n\n        if feature_probability is None:\n            feature_probability = t.ones(())\n        if isinstance(feature_probability, float):\n            feature_probability = t.tensor(feature_probability)\n        assert isinstance(\n            feature_probability, Tensor\n        )  # pyright can't seem to infer this\n        self.feature_probability = feature_probability.to(device).broadcast_to(\n            (cfg.n_instances, cfg.n_features)\n        )\n        if importance is None:\n            importance = t.ones(())\n        if isinstance(importance, float):\n            importance = t.tensor(importance)\n        assert isinstance(importance, Tensor)  # pyright can't seem to infer this\n        self.importance = importance.to(device).broadcast_to(\n            (cfg.n_instances, cfg.n_features)\n        )\n\n        self.W = nn.Parameter(\n            nn.init.xavier_normal_(\n                t.empty((cfg.n_instances, cfg.n_hidden, cfg.n_features))\n            )\n        )\n        self.b_final = nn.Parameter(t.zeros((cfg.n_instances, cfg.n_features)))\n        self.to(device)\n\n    def forward(\n        self, features: Float[Tensor, \"... instances features\"]\n    ) -&gt; Float[Tensor, \"... instances features\"]:\n        hidden = einops.einsum(\n            features,\n            self.W,\n            \"... instances features, instances hidden features -&gt; ... instances hidden\",\n        )\n        out = einops.einsum(\n            hidden,\n            self.W,\n            \"... instances hidden, instances hidden features -&gt; ... instances features\",\n        )\n        return F.relu(out + self.b_final)\n\n    # def generate_batch(self, batch_size) -&gt; Float[Tensor, \"batch_size instances features\"]:\n    #     '''\n    #     Generates a batch of data. We'll return to this function later when we apply correlations.\n    #     '''\n    #     feat = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W.device)\n    #     feat_seeds = t.rand((batch_size, self.cfg.n_instances, self.cfg.n_features), device=self.W.device)\n    #     feat_is_present = feat_seeds &lt;= self.feature_probability\n    #     batch = t.where(\n    #         feat_is_present,\n    #         feat,\n    #         t.zeros((), device=self.W.device),\n    #     )\n    #     return batch\n\n    def generate_correlated_features(\n        self, batch_size: int, n_correlated_pairs: int\n    ) -&gt; Float[Tensor, \"batch_size instances features\"]:\n        \"\"\"\n        Generates a batch of correlated features.\n        Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.\n        \"\"\"\n        feat = t.rand(\n            (batch_size, self.cfg.n_instances, 2 * n_correlated_pairs),\n            device=self.W.device,\n        )\n        feat_set_seeds = t.rand(\n            (batch_size, self.cfg.n_instances, n_correlated_pairs), device=self.W.device\n        )\n        feat_set_is_present = feat_set_seeds &lt;= self.feature_probability[:, [0]]\n        feat_is_present = einops.repeat(\n            feat_set_is_present,\n            \"batch instances features -&gt; batch instances (features pair)\",\n            pair=2,\n        )\n        return t.where(feat_is_present, feat, 0.0)\n\n    def generate_anticorrelated_features(\n        self, batch_size: int, n_anticorrelated_pairs: int\n    ) -&gt; Float[Tensor, \"batch_size instances features\"]:\n        \"\"\"\n        Generates a batch of anti-correlated features.\n        Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.\n        \"\"\"\n        feat = t.rand(\n            (batch_size, self.cfg.n_instances, 2 * n_anticorrelated_pairs),\n            device=self.W.device,\n        )\n        feat_set_seeds = t.rand(\n            (batch_size, self.cfg.n_instances, n_anticorrelated_pairs),\n            device=self.W.device,\n        )\n        first_feat_seeds = t.rand(\n            (batch_size, self.cfg.n_instances, n_anticorrelated_pairs),\n            device=self.W.device,\n        )\n        feat_set_is_present = feat_set_seeds &lt;= 2 * self.feature_probability[:, [0]]\n        first_feat_is_present = first_feat_seeds &lt;= 0.5\n        first_feats = t.where(\n            feat_set_is_present &amp; first_feat_is_present,\n            feat[:, :, :n_anticorrelated_pairs],\n            0.0,\n        )\n        second_feats = t.where(\n            feat_set_is_present &amp; (~first_feat_is_present),\n            feat[:, :, n_anticorrelated_pairs:],\n            0.0,\n        )\n        return einops.rearrange(\n            t.concat([first_feats, second_feats], dim=-1),\n            \"batch instances (pair features) -&gt; batch instances (features pair)\",\n            pair=2,\n        )\n\n    def generate_uncorrelated_features(\n        self, batch_size: int, n_uncorrelated: int\n    ) -&gt; Float[Tensor, \"batch_size instances features\"]:\n        \"\"\"\n        Generates a batch of uncorrelated features.\n        \"\"\"\n        feat = t.rand(\n            (batch_size, self.cfg.n_instances, n_uncorrelated), device=self.W.device\n        )\n        feat_seeds = t.rand(\n            (batch_size, self.cfg.n_instances, n_uncorrelated), device=self.W.device\n        )\n        feat_is_present = feat_seeds &lt;= self.feature_probability[:, [0]]\n        return t.where(feat_is_present, feat, 0.0)\n\n    def generate_batch(\n        self, batch_size: int\n    ) -&gt; Float[Tensor, \"batch_size instances features\"]:\n        \"\"\"\n        Generates a batch of data, with optional correlated &amp; anticorrelated features.\n        \"\"\"\n        n_uncorrelated = (\n            self.cfg.n_features\n            - 2 * self.cfg.n_correlated_pairs\n            - 2 * self.cfg.n_anticorrelated_pairs\n        )\n        data = []\n        if self.cfg.n_correlated_pairs &gt; 0:\n            data.append(\n                self.generate_correlated_features(\n                    batch_size, self.cfg.n_correlated_pairs\n                )\n            )\n        if self.cfg.n_anticorrelated_pairs &gt; 0:\n            data.append(\n                self.generate_anticorrelated_features(\n                    batch_size, self.cfg.n_anticorrelated_pairs\n                )\n            )\n        if n_uncorrelated &gt; 0:\n            data.append(self.generate_uncorrelated_features(batch_size, n_uncorrelated))\n        batch = t.cat(data, dim=-1)\n        return batch\n\n    def calculate_loss(\n        self,\n        out: Float[Tensor, \"batch instances features\"],\n        batch: Float[Tensor, \"batch instances features\"],\n    ) -&gt; Float[Tensor, \"\"]:\n        \"\"\"\n        Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n\n            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n\n        Note, `model.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n        \"\"\"\n        error = self.importance * ((batch - out) ** 2)\n        loss = einops.reduce(\n            error, \"batch instances features -&gt; instances\", \"mean\"\n        ).sum()\n        return loss\n\n    def optimize(\n        self,\n        batch_size: int = 1024,\n        steps: int = 10_000,\n        log_freq: int = 100,\n        lr: float = 1e-3,\n        lr_scale: Callable[[int, int], float] = constant_lr,\n    ):\n        \"\"\"\n        Optimizes the model using the given hyperparameters.\n        \"\"\"\n        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n\n        progress_bar = tqdm(range(steps), desc=\"Training Toy Model\")\n\n        for step in progress_bar:\n            # Update learning rate\n            step_lr = lr * lr_scale(step, steps)\n            for group in optimizer.param_groups:\n                group[\"lr\"] = step_lr\n\n            # Optimize\n            optimizer.zero_grad()\n            batch = self.generate_batch(batch_size)\n            out = self(batch)\n            loss = self.calculate_loss(out, batch)\n            loss.backward()\n            optimizer.step()\n\n            # Display progress bar\n            if step % log_freq == 0 or (step + 1 == steps):\n                progress_bar.set_postfix(\n                    loss=loss.item() / self.cfg.n_instances, lr=step_lr\n                )\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.Model.calculate_loss","title":"<code>calculate_loss(out, batch)</code>","text":"<p>Calculates the loss for a given batch, using this loss described in the Toy Models paper:</p> <pre><code>https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n</code></pre> <p>Note, <code>model.importance</code> is guaranteed to broadcast with the shape of <code>out</code> and <code>batch</code>.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def calculate_loss(\n    self,\n    out: Float[Tensor, \"batch instances features\"],\n    batch: Float[Tensor, \"batch instances features\"],\n) -&gt; Float[Tensor, \"\"]:\n    \"\"\"\n    Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n\n        https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n\n    Note, `model.importance` is guaranteed to broadcast with the shape of `out` and `batch`.\n    \"\"\"\n    error = self.importance * ((batch - out) ** 2)\n    loss = einops.reduce(\n        error, \"batch instances features -&gt; instances\", \"mean\"\n    ).sum()\n    return loss\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.Model.generate_anticorrelated_features","title":"<code>generate_anticorrelated_features(batch_size, n_anticorrelated_pairs)</code>","text":"<p>Generates a batch of anti-correlated features. Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_anticorrelated_features(\n    self, batch_size: int, n_anticorrelated_pairs: int\n) -&gt; Float[Tensor, \"batch_size instances features\"]:\n    \"\"\"\n    Generates a batch of anti-correlated features.\n    Each output[i, j, 2k] and output[i, j, 2k + 1] are anti-correlated, i.e. one is present iff the other is absent.\n    \"\"\"\n    feat = t.rand(\n        (batch_size, self.cfg.n_instances, 2 * n_anticorrelated_pairs),\n        device=self.W.device,\n    )\n    feat_set_seeds = t.rand(\n        (batch_size, self.cfg.n_instances, n_anticorrelated_pairs),\n        device=self.W.device,\n    )\n    first_feat_seeds = t.rand(\n        (batch_size, self.cfg.n_instances, n_anticorrelated_pairs),\n        device=self.W.device,\n    )\n    feat_set_is_present = feat_set_seeds &lt;= 2 * self.feature_probability[:, [0]]\n    first_feat_is_present = first_feat_seeds &lt;= 0.5\n    first_feats = t.where(\n        feat_set_is_present &amp; first_feat_is_present,\n        feat[:, :, :n_anticorrelated_pairs],\n        0.0,\n    )\n    second_feats = t.where(\n        feat_set_is_present &amp; (~first_feat_is_present),\n        feat[:, :, n_anticorrelated_pairs:],\n        0.0,\n    )\n    return einops.rearrange(\n        t.concat([first_feats, second_feats], dim=-1),\n        \"batch instances (pair features) -&gt; batch instances (features pair)\",\n        pair=2,\n    )\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.Model.generate_batch","title":"<code>generate_batch(batch_size)</code>","text":"<p>Generates a batch of data, with optional correlated &amp; anticorrelated features.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_batch(\n    self, batch_size: int\n) -&gt; Float[Tensor, \"batch_size instances features\"]:\n    \"\"\"\n    Generates a batch of data, with optional correlated &amp; anticorrelated features.\n    \"\"\"\n    n_uncorrelated = (\n        self.cfg.n_features\n        - 2 * self.cfg.n_correlated_pairs\n        - 2 * self.cfg.n_anticorrelated_pairs\n    )\n    data = []\n    if self.cfg.n_correlated_pairs &gt; 0:\n        data.append(\n            self.generate_correlated_features(\n                batch_size, self.cfg.n_correlated_pairs\n            )\n        )\n    if self.cfg.n_anticorrelated_pairs &gt; 0:\n        data.append(\n            self.generate_anticorrelated_features(\n                batch_size, self.cfg.n_anticorrelated_pairs\n            )\n        )\n    if n_uncorrelated &gt; 0:\n        data.append(self.generate_uncorrelated_features(batch_size, n_uncorrelated))\n    batch = t.cat(data, dim=-1)\n    return batch\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.Model.generate_correlated_features","title":"<code>generate_correlated_features(batch_size, n_correlated_pairs)</code>","text":"<p>Generates a batch of correlated features. Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_correlated_features(\n    self, batch_size: int, n_correlated_pairs: int\n) -&gt; Float[Tensor, \"batch_size instances features\"]:\n    \"\"\"\n    Generates a batch of correlated features.\n    Each output[i, j, 2k] and output[i, j, 2k + 1] are correlated, i.e. one is present iff the other is present.\n    \"\"\"\n    feat = t.rand(\n        (batch_size, self.cfg.n_instances, 2 * n_correlated_pairs),\n        device=self.W.device,\n    )\n    feat_set_seeds = t.rand(\n        (batch_size, self.cfg.n_instances, n_correlated_pairs), device=self.W.device\n    )\n    feat_set_is_present = feat_set_seeds &lt;= self.feature_probability[:, [0]]\n    feat_is_present = einops.repeat(\n        feat_set_is_present,\n        \"batch instances features -&gt; batch instances (features pair)\",\n        pair=2,\n    )\n    return t.where(feat_is_present, feat, 0.0)\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.Model.generate_uncorrelated_features","title":"<code>generate_uncorrelated_features(batch_size, n_uncorrelated)</code>","text":"<p>Generates a batch of uncorrelated features.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def generate_uncorrelated_features(\n    self, batch_size: int, n_uncorrelated: int\n) -&gt; Float[Tensor, \"batch_size instances features\"]:\n    \"\"\"\n    Generates a batch of uncorrelated features.\n    \"\"\"\n    feat = t.rand(\n        (batch_size, self.cfg.n_instances, n_uncorrelated), device=self.W.device\n    )\n    feat_seeds = t.rand(\n        (batch_size, self.cfg.n_instances, n_uncorrelated), device=self.W.device\n    )\n    feat_is_present = feat_seeds &lt;= self.feature_probability[:, [0]]\n    return t.where(feat_is_present, feat, 0.0)\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.Model.optimize","title":"<code>optimize(batch_size=1024, steps=10000, log_freq=100, lr=0.001, lr_scale=constant_lr)</code>","text":"<p>Optimizes the model using the given hyperparameters.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def optimize(\n    self,\n    batch_size: int = 1024,\n    steps: int = 10_000,\n    log_freq: int = 100,\n    lr: float = 1e-3,\n    lr_scale: Callable[[int, int], float] = constant_lr,\n):\n    \"\"\"\n    Optimizes the model using the given hyperparameters.\n    \"\"\"\n    optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n\n    progress_bar = tqdm(range(steps), desc=\"Training Toy Model\")\n\n    for step in progress_bar:\n        # Update learning rate\n        step_lr = lr * lr_scale(step, steps)\n        for group in optimizer.param_groups:\n            group[\"lr\"] = step_lr\n\n        # Optimize\n        optimizer.zero_grad()\n        batch = self.generate_batch(batch_size)\n        out = self(batch)\n        loss = self.calculate_loss(out, batch)\n        loss.backward()\n        optimizer.step()\n\n        # Display progress bar\n        if step % log_freq == 0 or (step + 1 == steps):\n            progress_bar.set_postfix(\n                loss=loss.item() / self.cfg.n_instances, lr=step_lr\n            )\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.parse_colors_for_superposition_plot","title":"<code>parse_colors_for_superposition_plot(colors, n_instances, n_feats)</code>","text":"<p>There are lots of different ways colors can be represented in the superposition plot.</p> <p>This function unifies them all by turning colors into a list of lists of strings, i.e. one color for each instance &amp; feature.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def parse_colors_for_superposition_plot(\n    colors: Optional[\n        Union[Tuple[int, int], List[List[str]], Float[Tensor, \"instances feats\"]]\n    ],\n    n_instances: int,\n    n_feats: int,\n) -&gt; List[List[str]]:\n    \"\"\"\n    There are lots of different ways colors can be represented in the superposition plot.\n\n    This function unifies them all by turning colors into a list of lists of strings, i.e. one color for each instance &amp; feature.\n    \"\"\"\n    # If colors is a tensor, we assume it's the importances tensor, and we color according to a viridis color scheme\n    # if isinstance(colors, Tensor):\n    #     colors = t.broadcast_to(colors, (n_instances, n_feats))\n    #     colors = [\n    #         [helper_get_viridis(v.item()) for v in colors_for_this_instance]\n    #         for colors_for_this_instance in colors\n    #     ]\n\n    # If colors is a tuple of ints, it's interpreted as number of correlated / anticorrelated pairs\n    if isinstance(colors, tuple):\n        n_corr, n_anti = colors\n        n_indep = n_feats - 2 * (n_corr - n_anti)\n        return [\n            [\"blue\", \"blue\", \"limegreen\", \"limegreen\", \"purple\", \"purple\"][: n_corr * 2]\n            + [\"red\", \"red\", \"orange\", \"orange\", \"brown\", \"brown\"][: n_anti * 2]\n            + [\"black\"] * n_indep\n            for _ in range(n_instances)\n        ]\n\n    # If colors is a string, make all datapoints that color\n    elif isinstance(colors, str):\n        return [[colors] * n_feats] * n_instances\n\n    # Lastly, if colors is None, make all datapoints black\n    elif colors is None:\n        return [[\"black\"] * n_feats] * n_instances\n\n    assert isinstance(colors, list)\n    return colors\n</code></pre>"},{"location":"reference/toy_models/#sae_lens.training.toy_models.plot_features_in_2d","title":"<code>plot_features_in_2d(values, colors=None, title=None, subplot_titles=None, save=None, colab=False)</code>","text":"<p>Visualises superposition in 2D.</p> <p>If values is 4D, the first dimension is assumed to be timesteps, and an animation is created.</p> Source code in <code>sae_lens/training/toy_models.py</code> <pre><code>def plot_features_in_2d(\n    values: Float[Tensor, \"timesteps instances d_hidden feats\"],\n    colors: Optional[list[Any]] = None,  # shape [timesteps instances feats]\n    title: Optional[str | list[str]] = None,\n    subplot_titles: Optional[list[str] | list[list[str]]] = None,\n    save: Optional[str] = None,\n    colab: bool = False,\n):\n    \"\"\"\n    Visualises superposition in 2D.\n\n    If values is 4D, the first dimension is assumed to be timesteps, and an animation is created.\n    \"\"\"\n    # Convert values to 4D for consistency\n    if values.ndim == 3:\n        values = values.unsqueeze(0)\n    values = values.transpose(-1, -2)\n\n    # Get dimensions\n    n_timesteps, n_instances, n_features, _ = values.shape\n\n    # If we have a large number of features per plot (i.e. we're plotting projections of data) then use smaller lines\n    linewidth, markersize = (1, 4) if (n_features &gt;= 25) else (2, 10)\n\n    # Convert colors to 3D, if it's 2D (i.e. same colors for all instances)\n    if isinstance(colors, list) and isinstance(colors[0], str):\n        colors = [colors for _ in range(n_instances)]\n    # Convert colors to something which has 4D, if it is 3D (i.e. same colors for all timesteps)\n    if any(\n        [\n            colors is None,\n            isinstance(colors, list)\n            and isinstance(colors[0], list)\n            and isinstance(colors[0][0], str),\n            (isinstance(colors, Tensor) or isinstance(colors, Arr))\n            and colors.ndim == 3,\n        ]\n    ):\n        colors = [colors for _ in range(values.shape[0])]\n    # Now that colors has length `timesteps` in some sense, we can convert it to lists of strings\n    assert colors is not None  # keep pyright happy\n    colors = [\n        parse_colors_for_superposition_plot(c, n_instances, n_features) for c in colors\n    ]\n\n    # Same for subplot titles &amp; titles\n    if subplot_titles is not None:\n        if isinstance(subplot_titles, list) and isinstance(subplot_titles[0], str):\n            subplot_titles = [\n                cast(list[str], subplot_titles) for _ in range(values.shape[0])\n            ]\n    if title is not None:\n        if isinstance(title, str):\n            title = [title for _ in range(values.shape[0])]\n\n    # Create a figure and axes\n    fig, axs = plt.subplots(1, n_instances, figsize=(5 * n_instances, 5))\n    if n_instances == 1:\n        axs = [axs]\n\n    # If there are titles, add more spacing for them\n    fig.subplots_adjust(bottom=0.2, top=0.9, left=0.1, right=0.9)\n    if title:\n        fig.subplots_adjust(top=0.8)\n    # Initialize lines and markers\n    lines = []\n    markers = []\n    for instance_idx, ax in enumerate(axs):\n        ax.set_xlim(-1.5, 1.5)\n        ax.set_ylim(-1.5, 1.5)\n        ax.set_aspect(\"equal\", adjustable=\"box\")\n        instance_lines = []\n        instance_markers = []\n        for feature_idx in range(n_features):\n            (line,) = ax.plot(\n                [], [], color=colors[0][instance_idx][feature_idx], lw=linewidth\n            )\n            (marker,) = ax.plot(\n                [],\n                [],\n                color=colors[0][instance_idx][feature_idx],\n                marker=\"o\",\n                markersize=markersize,\n            )\n            instance_lines.append(line)\n            instance_markers.append(marker)\n        lines.append(instance_lines)\n        markers.append(instance_markers)\n\n    def update(val: float):\n        # I think this doesn't work unless I at least reference the nonlocal slider object\n        # It works if I use t = int(val), so long as I put something like X = slider.val first. Idk why!\n        if n_timesteps &gt; 1:\n            _ = slider.val\n        t = int(val)\n        for instance_idx in range(n_instances):\n            for feature_idx in range(n_features):\n                x, y = values[t, instance_idx, feature_idx].tolist()\n                lines[instance_idx][feature_idx].set_data([0, x], [0, y])\n                markers[instance_idx][feature_idx].set_data(x, y)\n                lines[instance_idx][feature_idx].set_color(\n                    colors[t][instance_idx][feature_idx]\n                )\n                markers[instance_idx][feature_idx].set_color(\n                    colors[t][instance_idx][feature_idx]\n                )\n            if title:\n                fig.suptitle(title[t], fontsize=15)\n            if subplot_titles:\n                axs[instance_idx].set_title(\n                    subplot_titles[t][instance_idx], fontsize=12\n                )\n        fig.canvas.draw_idle()\n\n    def play(event: Any):\n        _ = slider.val\n        for i in range(n_timesteps):\n            update(i)\n            slider.set_val(i)\n            plt.pause(0.05)\n        fig.canvas.draw_idle()\n\n    if n_timesteps &gt; 1:\n        # Create the slider\n        ax_slider = plt.axes((0.15, 0.05, 0.7, 0.05), facecolor=\"lightgray\")\n        slider = Slider(\n            ax_slider, \"Time\", 0, n_timesteps - 1, valinit=0, valfmt=\"%1.0f\"\n        )\n\n        # Create the play button\n        # ax_button = plt.axes([0.8, 0.05, 0.08, 0.05], facecolor='lightgray')\n        # button = Button(ax_button, 'Play')\n\n        # Call the update function when the slider value is changed / button is clicked\n        slider.on_changed(update)\n        # button.on_clicked(play)\n\n        # Initialize the plot\n        play(0)\n    else:\n        update(0)\n\n    # Save\n    if isinstance(save, str):\n        ani = FuncAnimation(\n            fig, cast(Any, update), frames=n_timesteps, interval=0.04, repeat=False\n        )\n        ani.save(save, writer=\"pillow\", fps=25)\n    elif colab:\n        ani = FuncAnimation(\n            fig, cast(Any, update), frames=n_timesteps, interval=0.04, repeat=False\n        )\n        clear_output()\n        return ani\n</code></pre>"}]}